{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d6c32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorchtools\n",
    "import glob as gl\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.datasets import make_regression\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import log_loss\n",
    "from numpy import hstack\n",
    "from numpy import vstack\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d861f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_directory(foldername, filename = None, back_num = 0):\n",
    "    cur = os.getcwd()\n",
    "    for i in range(back_num):\n",
    "        cur = os.path.abspath(os.path.join(cur, os.pardir))\n",
    "    for folder in foldername:\n",
    "        cur = os.path.join(cur, folder)\n",
    "    if not os.path.exists(cur):\n",
    "        os.makedirs(cur)\n",
    "        print(f'{cur} created')\n",
    "    if filename != None:\n",
    "        cur = os.path.join(cur, filename)\n",
    "    return cur\n",
    "\n",
    "os.getcwd()\n",
    "find_directory(back_num = 1, foldername = ['Dataset'], filename = 'bat_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475874d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bat_dict_add_1_2 = find_directory(back_num = 2, foldername = ['Dataset'], filename = 'sort_charge_dict_1_2.pkl')\n",
    "with open(bat_dict_add_1_2, 'rb') as tf:\n",
    "    bat_dict_1_2 = pickle.load(tf)\n",
    "\n",
    "bat_sel_dict = bat_dict_1_2\n",
    "max_time = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1331f44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bat_sel_dict['b1c0']['1'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea76ff",
   "metadata": {},
   "source": [
    "## Split per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3e9362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch1: 0~40(41)\n",
    "# batch2: 41~83(43)\n",
    "# batch3: 64~123(40)\n",
    "num_cells = len(bat_sel_dict.keys())\n",
    "using_vars = ['I', 'Qc', 'T', 'V']\n",
    "\n",
    "b1 = 0\n",
    "b2 = 0\n",
    "b3 = 0\n",
    "\n",
    "for cell in bat_sel_dict.keys():\n",
    "    if cell.startswith('b1'):\n",
    "        b1 += 1\n",
    "    elif cell.startswith('b2'):\n",
    "        b2 += 1\n",
    "    else:\n",
    "        b3 += 1\n",
    "\n",
    "print(b1, b2, b3)\n",
    "\n",
    "# Tensor dimension: (cell, cycle, time, variable)\n",
    "regression_cycles = [100]\n",
    "\n",
    "# variable = [Qc, I, V, T, Qd]\n",
    "num_vars = len(using_vars)\n",
    "\n",
    "for reg in regression_cycles:\n",
    "    globals()[f'x_tensor_{reg}'] = torch.zeros(num_cells, reg, max_time, num_vars)\n",
    "    cell_index = 1\n",
    "    for cell in bat_sel_dict.keys():\n",
    "        if cell.startswith('b1'):\n",
    "            for cycle in range(1, reg+1):\n",
    "                var_index = 0\n",
    "                for var in using_vars:\n",
    "                    value_list = bat_sel_dict[cell][str(cycle)][var]\n",
    "                    globals()[f'x_tensor_{reg}'][cell_index-1, cycle-1, :len(value_list), var_index] = torch.FloatTensor(value_list)\n",
    "                    var_index += 1\n",
    "            cell_index += 1\n",
    "        else:\n",
    "            for cycle in range(reg):\n",
    "                var_index = 0\n",
    "                for var in using_vars:\n",
    "                    value_list = bat_sel_dict[cell][str(cycle)][var]\n",
    "                    globals()[f'x_tensor_{reg}'][cell_index-1, cycle, :len(value_list), var_index] = torch.FloatTensor(value_list)\n",
    "                    var_index += 1\n",
    "            cell_index += 1\n",
    "\n",
    "    globals()[f'x_tensor_{reg}'].size()\n",
    "    \n",
    "    \n",
    "# globals()[f'x_tensor_b1_{reg}'] = globals()[f'x_tensor_{reg}'][:b1, :, :, :]\n",
    "# globals()[f'x_tensor_b2_{reg}'] = globals()[f'x_tensor_{reg}'][b1:b1+b2, :, :, :]\n",
    "# globals()[f'x_tensor_b3_{reg}'] = globals()[f'x_tensor_{reg}'][b1+b2:, :, :, :]\n",
    "    \n",
    "# globals()[f'x_tensor_b1_{reg}'].size()\n",
    "# globals()[f'x_tensor_b2_{reg}'].size()\n",
    "# globals()[f'x_tensor_b3_{reg}'].size()\n",
    "\n",
    "num_variables = x_tensor_100.shape[3]\n",
    "time_length = x_tensor_100.shape[2]\n",
    "# (cycle_index-1, cycle-1, time-1, variable(Qc, I, V, T, Qd))\n",
    "# x_tensor[123, 99, 30, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b6d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df_add = find_directory(back_num = 2, foldername = ['Dataset', 'y_df.csv'])\n",
    "y_df = pd.read_csv(y_df_add)\n",
    "\n",
    "y_cl_numpy = y_df['Cycle life'].to_numpy()\n",
    "y_kp_numpy = y_df['kneepoints'].to_numpy()\n",
    "y_ko_numpy = y_df['kneeonsets'].to_numpy()\n",
    "\n",
    "y_regression_numpy = y_ko_numpy\n",
    "\n",
    "y_ko_tensor = torch.FloatTensor(y_ko_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e414a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setRandomSeed(random_seed=0):\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "    torch.manual_seed(random_seed) # torch \n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True # cudnn\n",
    "    torch.backends.cudnn.benchmark = False # cudnn\n",
    "    np.random.seed(random_seed) # numpy\n",
    "    random.seed(random_seed) # random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea834cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y: no scaling\n",
    "class dataPrep_RNN_CNN(Dataset):  \n",
    "    def __init__(self, x_tensor, y_tensor, batch_size, scaler, b1, b2, b3):\n",
    "        setRandomSeed(random_seed = 100)\n",
    "        self.xdata = torch.permute(x_tensor, (0, 3, 2, 1))\n",
    "        self.ydata = y_tensor\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "                \n",
    "        self.xscaler = scaler()\n",
    "        \n",
    "        # Tensor dimension: (cell, variable, time, cycle)\n",
    "        # If problem emerges, check this part first.\n",
    "        for i in range(len(self.xdata)):\n",
    "            for j in range(self.xdata.shape[1]):\n",
    "                temp = np.expand_dims(self.xscaler.fit_transform(self.xdata[i,j,:,:]), axis=0)\n",
    "                if j ==0:\n",
    "                    temp2 = temp\n",
    "                else:\n",
    "                    temp2 = np.vstack((temp2,temp))\n",
    "            temp2 = np.expand_dims(temp2, axis=0)\n",
    "            if i==0:\n",
    "                self.xdata_scaled = temp2\n",
    "            else:\n",
    "                self.xdata_scaled = np.vstack((self.xdata_scaled, temp2))\n",
    "\n",
    "        self.ydata_scaled = self.ydata\n",
    "        \n",
    "        self.all = list(zip(self.xdata_scaled, self.ydata_scaled))\n",
    "        \n",
    "        self.b1 = self.all[:b1]\n",
    "        self.b2 = self.all[b1:b1+b2]\n",
    "        self.b3 = self.all[b1+b2:]\n",
    "        \n",
    "        self.b1_dataloader = DataLoader(self.b1, batch_size = self.batch_size, shuffle = True)\n",
    "        self.b2_dataloader = DataLoader(self.b2, batch_size = self.batch_size, shuffle = True)\n",
    "        self.b3_dataloader = DataLoader(self.b3, batch_size = self.batch_size, shuffle = True)\n",
    "        \n",
    "        print(f\"batch 1: {len(self.b1)}, batch 2: {len(self.b2)}, batch 3: {len(self.b3)}\")\n",
    "    \n",
    "    def batch_dataloader(self):\n",
    "        return self.b1_dataloader, self.b2_dataloader, self.b3_dataloader\n",
    "    \n",
    "    def scaler(self):\n",
    "        return self.xscaler\n",
    "    \n",
    "    # finding length of x \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    # indexing rows for calling\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8767cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_cycles = [100]\n",
    "\n",
    "\n",
    "# x와 y가 1대1 대응이어야 dataloader를 만들 수 있어서 2D array 그대로 놓고 학습시 tensor transform 예정\n",
    "for reg in regression_cycles:\n",
    "    globals()[f'reg_dataset_{reg}'] = dataPrep_RNN_CNN(globals()[f'x_tensor_{reg}'], y_ko_tensor, 4, MinMaxScaler, b1, b2, b3)\n",
    "    globals()[f'b1_dataloader_{reg}'], globals()[f'b2_dataloader_{reg}'], globals()[f'b3_dataloader_{reg}'] = globals()[f'reg_dataset_{reg}'].batch_dataloader()\n",
    "    globals()[f'reg_xscaler_{reg}'] = globals()[f'reg_dataset_{reg}'].scaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7600cae",
   "metadata": {},
   "source": [
    "## Define model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce39f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNcell(nn.Module):\n",
    "    def __init__(self, model, input_size, hidden_size, dropout = 0.1, bidirectional = False, num_layer=1):\n",
    "        super(RNNcell, self).__init__()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "            \n",
    "        self.model = model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layer = num_layer\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.D = 1 + self.bidirectional\n",
    "        self.batch_size = 4\n",
    "        \n",
    "        #Batch size * cycles, timestep, input_size\n",
    "        if self.model.endswith('RNN'):\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layer, batch_first = True, dropout = dropout, bidirectional = bidirectional).to(self.device)\n",
    "        elif self.model.endswith('LSTM'):\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layer, batch_first = True, dropout = dropout, bidirectional = bidirectional).to(self.device)\n",
    "        elif self.model.endswith('GRU'):\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layer, batch_first = True, dropout = dropout, bidirectional = bidirectional).to(self.device)\n",
    "            \n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if name.startswith('weight'):\n",
    "                nn.init.xavier_uniform(param)\n",
    "            else:\n",
    "                nn.init.normal(param)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x made from (cell * n_cy, timestep(seq_len), num_vars)\n",
    "        self.batch_size = x.size(0)\n",
    "        \n",
    "        D = 1 + self.bidirectional\n",
    "        #out: Containing output features h_t from the last layer of LSTM for each t(if bidirectional ==True: concat of forward and backward hidden states)\n",
    "        #h_n: Containing final hidden state in the sequence\n",
    "        h0 = torch.zeros(D*self.num_layer, self.batch_size, self.hidden_size).to(self.device)\n",
    "        if self.model.endswith('LSTM'):\n",
    "            c0 = torch.zeros(D*self.num_layer, self.batch_size, self.hidden_size).to(self.device)\n",
    "            # c_n: Containing final cell state in the sequence\n",
    "            out, (hn, cn) = self.rnn(x, (h0, c0))\n",
    "        else:\n",
    "            out, hn = self.rnn(x, h0.detach())\n",
    "        return out, hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386d060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_TA_1DCNN(nn.Module):\n",
    "    def __init__(self, input_size, num_time, num_cycles, rnn1, bi1, hid1, fil2, pool2, npool2, fsize2, psize2, mids, dr1 = 0.1, dr2 = 0.1, di2 = 1, st2 = 1, pad2 = 0):\n",
    "        super(RNN_TA_1DCNN, self).__init__()\n",
    "        setRandomSeed()\n",
    "        \n",
    "        self.D1 = 1 + bi1\n",
    "        \n",
    "        self.rnn1 = RNNcell(rnn1, input_size, hid1, dr1, bi1)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "        \n",
    "        # Overall parameters\n",
    "        self.num_time = num_time\n",
    "        self.num_cycles = num_cycles\n",
    "        self.mids = mids\n",
    "        \n",
    "        # Pooling/Nonpooling parameters\n",
    "        self.pool2 = pool2\n",
    "        self.npool2 = npool2\n",
    "        # Pooling size(Pooling layer)\n",
    "        self.psize2 = psize2\n",
    "        \n",
    "        # CNN hyperparameters\n",
    "        self.fil2 = fil2\n",
    "        self.fsize2 = fsize2\n",
    "        self.di2 = di2\n",
    "        self.st2 = st2\n",
    "        # Padding for CNN\n",
    "        self.pad2 = pad2\n",
    "                \n",
    "        # Critical values for the 1st CNN layer\n",
    "        # Batch for different cells\n",
    "        \n",
    "        # Conv1d takes input dimension: [Batch_size, c_in(no. of in_channels )= hidden_size, l_in = num_cycles]\n",
    "        self.in2 = self.D1*hid1\n",
    "        self.out2 = self.fil2\n",
    "        self.Lout = self.num_cycles\n",
    "        \n",
    "        # Conv_block1~P\n",
    "        # Pooling layers\n",
    "        for i in range(1, self.pool2+1):\n",
    "#             print(f\"i: {i}, out_channels: {self.out_channels}\")\n",
    "            globals()[f'self.conv_block{i}'] = nn.Sequential(\n",
    "            nn.Conv1d(self.in2, self.out2, self.fsize2, self.st2, self.pad2),\n",
    "            nn.BatchNorm1d(self.out2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(self.psize2)\n",
    "            ).to(self.device)\n",
    "\n",
    "            self.in2 = self.out2\n",
    "            self.out2 = self.out2*2\n",
    "            self.Lout = int(int((self.Lout+2*self.pad2-self.di2*(self.fsize2-1)-1)/self.st2+1)/self.psize2)\n",
    "\n",
    "        for j in range(self.pool2+1, self.pool2+self.npool2+1):\n",
    "            globals()[f'self.conv_block{j}'] = nn.Sequential(\n",
    "            nn.Conv1d(self.in2, self.out2, self.fsize2, self.st2, self.pad2),\n",
    "            nn.BatchNorm1d(self.out2),\n",
    "            nn.ReLU(),\n",
    "            ).to(self.device)\n",
    "            \n",
    "            self.in2 = self.out2\n",
    "            self.out2 = self.out2*2\n",
    "            self.Lout = int((self.Lout+2*self.pad2-self.di2*(self.fsize2-1)-1)/self.st2+1)\n",
    "        \n",
    "        self.lin1 = nn.Linear(self.D1*hid1, 1)\n",
    "        self.sm1 = nn.Softmax(dim = 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Linear(self.in2*self.Lout, self.mids[0], bias=True), nn.ReLU(),\n",
    "                       nn.Linear(self.mids[0], self.mids[1], bias = True), nn.ReLU(),\n",
    "                       nn.Linear(self.mids[1], self.mids[2], bias = True))\n",
    "        \n",
    "        self.fc.apply(self.init_weights)\n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        self.m = m\n",
    "        if type(self.m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(self.m.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Original: Batch size, num_vars, timestep, cycles=>Batch size * cycles, timestep, input_size\n",
    "        x = torch.reshape(x, (x.size(0)*x.size(3), x.size(2), x.size(1))).to(device)\n",
    "#         print(\"Batch size * cycles, timestep, input_size: \", x.size())\n",
    "        out1, hn1 = self.rnn1(x)\n",
    "        \n",
    "        # hn의 batch size가 n_cy*k로 설정\n",
    "        cell_batch_size = int(hn1.size(1)/self.num_cycles)\n",
    "\n",
    "        # Reshape for input\n",
    "        # out1: (batch_size = num_cell*num_cycle, timestep, D1*hid1)\n",
    "        # outs : (batch_size(num_cell), num_cycle, timestep, D1*hid1)\n",
    "        outs1 = torch.reshape(out1, (cell_batch_size, -1, out1.size(1), out1.size(2)))\n",
    "        # ta: (batch_size, num_cycle, timestep, 1(D1*hid1->1))\n",
    "        ta = self.sm1(self.lin1(outs1))\n",
    "        # ta_outs: (batch_size, num_cycle, timestep, D1*hid1)=>(batch_size, D1*hid1, num_cycle, timestep)\n",
    "        ta_outs = torch.permute(ta*outs1, (0, 3, 1, 2))\n",
    "        # ct_vec: (batch_size, D1*hid1, num_cycle)\n",
    "        ct_vec = torch.sum(ta_outs, -1)\n",
    "        \n",
    "        # input for 1d cnn: (batch_size, D1*hid1, num_cycle)\n",
    "        for i in range(1, self.pool2+self.npool2+1):\n",
    "            ct_vec = globals()[f'self.conv_block{i}'](ct_vec)\n",
    "        # ct_vec from final convolutional layer: (batch_size, self.in2*self.Lout)\n",
    "        ct_vec = ct_vec.view(ct_vec.size(0), -1)\n",
    "        # final_out: (batch_size, 1)\n",
    "        final_out = self.fc(ct_vec)\n",
    "        \n",
    "        return final_out.squeeze(), ta.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd9ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for saving and loading of training history\n",
    "def save_data(D3_array, filename):\n",
    "    with open(filename,\"wb\") as dat_:\n",
    "        pickle.dump(D3_array,dat_)\n",
    "        \n",
    "def load_data(filename):\n",
    "    with open(filename,\"rb\") as ld:\n",
    "        x_temp = pickle.load(ld)\n",
    "    return x_temp\n",
    "\n",
    "def history_state_dict_add(num_cycles, modelname, n_ep, patience, rnn1, hid1, fil2, pool2, npool2, fsize2, psize2, lr):\n",
    "    history_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, f'{rnn1}_1D CNN', 'train_history'], \n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}.pkl')\n",
    "    state_dict_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, f'{rnn1}_1D CNN', 'model'], \n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}_state_dict.pth')\n",
    "    ta_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, f'{rnn1}_1D CNN', 'ta'],\n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}_ta.wb')\n",
    "    return history_add, state_dict_add, ta_add\n",
    "\n",
    "\n",
    "def ta_per_batches_add(num_cycles, modelname, n_ep, patience, rnn1, hid1, fil2, pool2, npool2, fsize2, psize2, lr):\n",
    "    batch1_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, f'{rnn1}_1D CNN', 'ta_batch1'], \n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}.pkl')\n",
    "    batch2_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, f'{rnn1}_1D CNN', 'ta_batch2'], \n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}_state_dict.pth')\n",
    "    batch3_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, f'{rnn1}_1D CNN', 'ta_batch3'],\n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}_ta.wb')\n",
    "    fig_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, 'figures', 'ta'],\n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}.png')\n",
    "    return batch1_add, batch2_add, batch3_add, fig_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad009de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e13d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = 4\n",
    "modelname = 'RNN_TA_1DCNN'\n",
    "rnns = ['LSTM', 'GRU', 'RNN']\n",
    "hids = [3, 5, 7]\n",
    "num_time = max_time\n",
    "num_cycles = 100\n",
    "ep_pats = [[1000, 10], [2000, 20], [3000, 500]]\n",
    "lrs = [1e-4, 1e-3, 1e-2]\n",
    "\n",
    "num_fils = [3, 5, 7]\n",
    "pools = [1, 2]\n",
    "npools = [1, 2]\n",
    "fsize2 = 3\n",
    "psize2 = 2\n",
    "mids = [8, 4, 1]\n",
    "batch_size = 4\n",
    "\n",
    "case_num = 0\n",
    "case_test_under_100 = 0\n",
    "\n",
    "b1dl = globals()[f'b1_dataloader_{num_cycles}']\n",
    "b2dl = globals()[f'b2_dataloader_{num_cycles}']\n",
    "b3dl = globals()[f'b3_dataloader_{num_cycles}']\n",
    "dataloaders = [b1dl, b2dl, b3dl]\n",
    "batches = ['batch1', 'batch2', 'batch3']\n",
    "\n",
    "for [n_ep, patience] in ep_pats:\n",
    "    case_test_under_100_n_ep = 0\n",
    "    case_ep = 0\n",
    "    print(f\"epoch = {n_ep}, patience = {patience}\")\n",
    "    for rnn1, hid1 in itertools.product(rnns, hids):\n",
    "        bi1 = False\n",
    "        if rnn1.startswith('Bi'):\n",
    "            bi1 = True\n",
    "        for pool2, npool2 in itertools.product(pools, npools):\n",
    "            for fil2, lr in itertools.product(num_fils, lrs):\n",
    "                case_num += 1\n",
    "                history_add, state_dict_add, ta_add = history_state_dict_add(num_cycles, modelname, n_ep, patience,\n",
    "                                                                            rnn1, hid1, fil2, pool2, npool2, fsize2, psize2, lr)\n",
    "                \n",
    "                batch1_add, batch2_add, batch3_add, fig_add = ta_per_batches_add(num_cycles, modelname, n_ep, patience,\n",
    "                                                                rnn1, hid1, fil2, pool2, npool2, fsize2, psize2, lr)\n",
    "                \n",
    "                \n",
    "                model = globals()[modelname](num_vars, num_time, num_cycles, rnn1, bi1, hid1, fil2, pool2, npool2, fsize2, psize2, mids).to(device);\n",
    "                model.load_state_dict(load_data(state_dict_add));\n",
    "                \n",
    "                history = load_data(history_add);\n",
    "                rmse = [round(history.iloc[-1, 0],2), round(history.iloc[-1, 1], 2), round(history.iloc[-1, 2], 2)];\n",
    "                case_ep += 1;\n",
    "                if round(history.iloc[-1, 2], 2)<100:\n",
    "#                     print(f'{n_ep}, {patience}, {rnn1}, {hid1}, {pool2}, {npool2}, {fil2}, {lr}, {rmse}')\n",
    "                    case_test_under_100 += 1\n",
    "                    case_test_under_100_n_ep += 1\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        i = 0\n",
    "                        fig, axs = plt.subplots(1, 3, figsize = (30, 8));\n",
    "                        rows = 1\n",
    "                        cols = 3\n",
    "                        \n",
    "#                         b1_ta = torch.empty(batch_size, num_cycles, num_time).to(device)\n",
    "#                         b2_ta = torch.empty(batch_size, num_cycles, num_time).to(device)\n",
    "#                         b3_ta = torch.empty(batch_size, num_cycles, num_time).to(device)\n",
    "#                         tas = [b1_ta, b2_ta, b3_ta]\n",
    "                        for dataloader in dataloaders:\n",
    "                            b1_ta = torch.empty(batch_size, num_cycles, num_time).to(device)\n",
    "                            b2_ta = torch.empty(batch_size, num_cycles, num_time).to(device)\n",
    "                            b3_ta = torch.empty(batch_size, num_cycles, num_time).to(device)\n",
    "                            tas = [b1_ta, b2_ta, b3_ta]\n",
    "                            for n, (inputs, targets) in enumerate(dataloader):\n",
    "                                inputs = inputs.float().to(device)\n",
    "                                targets = targets.float().to(device)\n",
    "                                # ta: (batch_size, num_cycle, timestep)\n",
    "                                yhat, ta = model(inputs)\n",
    "                                if len(ta.size()) !=3:\n",
    "                                    ta = torch.unsqueeze(ta, 0)\n",
    "                    #                             print(ta.size())\n",
    "                                tas[i] = torch.cat((tas[i], ta), dim = 0)\n",
    "                            tas[i] = torch.mean(tas[i][batch_size:], dim = 0).squeeze()\n",
    "                            tas[i] = tas[i].detach().cpu().numpy();\n",
    "\n",
    "                            globals()[f'im_{i}'] = axs[i].imshow(tas[i], vmin = 0, vmax = 0.10, cmap = cm.gray, aspect = 'auto');\n",
    "\n",
    "                            axs[i].set_xlabel('Timestep', fontsize = 20);\n",
    "                            for t in axs[i].get_xticklabels():\n",
    "                                t.set_fontsize(20)\n",
    "                            axs[i].set_ylabel('Cycle', fontsize = 20);\n",
    "                            for t in axs[i].get_yticklabels():\n",
    "                                t.set_fontsize(20)\n",
    "                            axs[i].set_title(f'Batch {i+1} temporal attention score', fontsize = 25);\n",
    "                            i += 1\n",
    "\n",
    "                        fig.subplots_adjust(right = 0.85);\n",
    "\n",
    "                        i -= 1\n",
    "                    #     globals()[f'divider_{i}'] = make_axes_locatable(axs[i]);\n",
    "                    #     globals()[f'cax_{i}'] = globals()[f'divider_{i}'].append_axes(\"right\", size=\"10%\", pad=0.05);\n",
    "                        cbar_ax = fig.add_axes([0.88, 0.15, 0.02, 0.7])\n",
    "                    #     globals()[f'cbar_{i}'] = plt.colorbar(globals()[f'im_{i}'], cax = globals()[f'cax_{i}']);\n",
    "                        globals()[f'cbar_{i}'] = plt.colorbar(globals()[f'im_{i}'], cax = cbar_ax);\n",
    "\n",
    "                        for t in globals()[f'cbar_{i}'].ax.get_yticklabels():\n",
    "                            t.set_fontsize(20)\n",
    "                    #     plt.colorbar();\n",
    "\n",
    "                        fig.suptitle(f'Epoch: {n_ep}, Patience: {patience}, RNN: {rnn1}, Hidden size: {hid1}, Pool: {pool2}, Npool: {npool2}, Filter: {fil2}, lr: {lr}, RMSE:{rmse}', fontsize = 25);\n",
    "                        fig.show();\n",
    "                        plt.savefig(fig_add);                     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
