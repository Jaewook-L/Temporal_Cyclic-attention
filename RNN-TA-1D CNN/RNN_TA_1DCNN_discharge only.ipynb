{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e8ba63",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c62905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorchtools\n",
    "import glob as gl\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.datasets import make_regression\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import log_loss\n",
    "from numpy import hstack\n",
    "from numpy import vstack\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a16657",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5305e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_directory(foldername, filename = None, back_num = 0):\n",
    "    cur = os.getcwd()\n",
    "    for i in range(back_num):\n",
    "        cur = os.path.abspath(os.path.join(cur, os.pardir))\n",
    "    for folder in foldername:\n",
    "        cur = os.path.join(cur, folder)\n",
    "    if not os.path.exists(cur):\n",
    "        os.makedirs(cur)\n",
    "        print(f'{cur} created')\n",
    "    if filename != None:\n",
    "        cur = os.path.join(cur, filename)\n",
    "    return cur\n",
    "\n",
    "os.getcwd()\n",
    "find_directory(back_num = 2, foldername = ['Dataset'], filename = 'discharge_dict.pkl')\n",
    "\n",
    "# bat_dict_add = find_directory(back_num = 2, foldername = ['Dataset', 'bat_dict.pkl'])\n",
    "# with open(bat_dict_add, 'rb') as tf:\n",
    "#     bat_dict = pickle.load(tf)\n",
    "\n",
    "discharge_dict_add = find_directory(back_num = 2, foldername = ['Dataset'], filename = 'discharge_dict.pkl')\n",
    "with open(discharge_dict_add, 'rb') as tf:\n",
    "    bat_sel_dict = pickle.load(tf)\n",
    "# bat_sel_dict['b2c0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor dimension: (cell, cycle, time, variable)\n",
    "num_cells = len(bat_sel_dict.keys())\n",
    "regression_cycles = [100]\n",
    "max_time = 1000\n",
    "\n",
    "using_vars = ['dQdV', 'Qdlin', 'Tdlin']\n",
    "num_vars = len(using_vars)\n",
    "\n",
    "for reg in regression_cycles:\n",
    "    globals()[f'x_tensor_{reg}'] = torch.zeros(num_cells, reg, max_time, num_vars)\n",
    "    cell_index = 1\n",
    "    for cell in bat_sel_dict.keys():\n",
    "        for cycle in range(1, reg+1):\n",
    "            var_index = 0\n",
    "            for var in using_vars:\n",
    "                value_list = bat_sel_dict[cell][str(cycle)][var]\n",
    "                globals()[f'x_tensor_{reg}'][cell_index-1, cycle-1, :len(value_list), var_index] = torch.FloatTensor(value_list)\n",
    "                var_index += 1\n",
    "        cell_index += 1\n",
    "\n",
    "    globals()[f'x_tensor_{reg}'].size()\n",
    "\n",
    "num_variables = x_tensor_100.shape[3]\n",
    "time_length = x_tensor_100.shape[2]\n",
    "# (cycle_index-1, cycle-1, time-1, variable(Qc, I, V, T, Qd))\n",
    "# x_tensor[123, 99, 30, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a895844",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df_add = find_directory(back_num = 2, foldername = ['Dataset', 'y_df.csv'])\n",
    "y_df = pd.read_csv(y_df_add)\n",
    "\n",
    "# 2. Regression problem\n",
    "# KO, KP, CL\n",
    "y_cl_numpy = y_df['Cycle life'].to_numpy()\n",
    "y_kp_numpy = y_df['kneepoints'].to_numpy()\n",
    "y_ko_numpy = y_df['kneeonsets'].to_numpy()\n",
    "# y_ko_numpy.shape\n",
    "\n",
    "y_regression_numpy = y_ko_numpy\n",
    "\n",
    "y_ko_tensor = torch.FloatTensor(y_ko_numpy)\n",
    "y_ko_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6886ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setRandomSeed(random_seed=0):\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "    torch.manual_seed(random_seed) # torch \n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True # cudnn\n",
    "    torch.backends.cudnn.benchmark = False # cudnn\n",
    "    np.random.seed(random_seed) # numpy\n",
    "    random.seed(random_seed) # random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df19c8fd",
   "metadata": {},
   "source": [
    "### Dataloading as 2D CNN's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec0345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y: no scaling\n",
    "class dataPrep_RNN_CNN(Dataset):  \n",
    "    def __init__(self, x_tensor, y_tensor, batch_size, scaler, test_split: float):\n",
    "        setRandomSeed(random_seed = 100)\n",
    "        self.xdata = torch.permute(x_tensor, (0, 3, 2, 1))\n",
    "        self.ydata = y_tensor\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "                \n",
    "        self.xscaler = scaler()\n",
    "        \n",
    "        self.test_split = test_split\n",
    "        \n",
    "        self.train_size, self.val_size, self.test_size = self.get_splits()\n",
    "        \n",
    "        # Tensor dimension: (cell, variable, time, cycle)\n",
    "        # If problem emerges, check this part first.\n",
    "        for i in range(len(self.xdata)):\n",
    "            for j in range(self.xdata.shape[1]):\n",
    "                temp = np.expand_dims(self.xscaler.fit_transform(self.xdata[i,j,:,:]), axis=0)\n",
    "                if j ==0:\n",
    "                    temp2 = temp\n",
    "                else:\n",
    "                    temp2 = np.vstack((temp2,temp))\n",
    "            temp2 = np.expand_dims(temp2, axis=0)\n",
    "            if i==0:\n",
    "                self.xdata_scaled = temp2\n",
    "            else:\n",
    "                self.xdata_scaled = np.vstack((self.xdata_scaled, temp2))\n",
    "\n",
    "        self.ydata_scaled = self.ydata\n",
    "        \n",
    "        self.x_train_scaled, self.x_test_scaled, self.y_train_scaled, self.y_test_scaled = train_test_split(self.xdata_scaled, \n",
    "                                                                                                            self.ydata_scaled, \n",
    "                                                                                                            test_size = self.test_size, \n",
    "                                                                                                            shuffle = True, \n",
    "                                                                                                            random_state=100)\n",
    "        \n",
    "        self.x_train_scaled, self.x_val_scaled, self.y_train_scaled, self.y_val_scaled = train_test_split(self.x_train_scaled,\n",
    "                                                                                                         self.y_train_scaled,\n",
    "                                                                                                         test_size = self.val_size, \n",
    "                                                                                                         shuffle = True, \n",
    "                                                                                                         random_state = 100)\n",
    "        \n",
    "        print(self.x_train_scaled.shape, self.x_val_scaled.shape, self.x_test_scaled.shape,\n",
    "              self.y_train_scaled.shape, self.y_val_scaled.shape, self.y_test_scaled.shape)\n",
    "        \n",
    "        self.train = list(zip(self.x_train_scaled, self.y_train_scaled))\n",
    "        self.val = list(zip(self.x_val_scaled, self.y_val_scaled))\n",
    "        self.test = list(zip(self.x_test_scaled, self.y_test_scaled))\n",
    "                \n",
    "        self.train_dataloader = DataLoader(self.train, batch_size = self.batch_size, shuffle=True)\n",
    "        self.val_dataloader = DataLoader(self.val, batch_size = self.batch_size, shuffle=True)\n",
    "        self.test_dataloader = DataLoader(self.test, batch_size = self.batch_size, shuffle=True)\n",
    "            \n",
    "    # split the datasets into training and testing\n",
    "    def get_splits(self):\n",
    "        # test size\n",
    "        test_size = int(self.test_split*len(self.xdata))\n",
    "        # val size\n",
    "        val_size = int(0.2*(len(self.xdata) - test_size))\n",
    "        # train_size\n",
    "        train_size = len(self.xdata) - val_size - test_size\n",
    "        return train_size, val_size, test_size\n",
    "    \n",
    "    # Returns dataloaders for training and validation datasets\n",
    "    def train_data(self):\n",
    "        return self.train_dataloader, self.val_dataloader\n",
    "    \n",
    "    # Returns dataloader for test datasets\n",
    "    def test_data(self):\n",
    "        return self.test_dataloader\n",
    "    \n",
    "    def scaler(self):\n",
    "#         if purpose != 'classification':\n",
    "#             return self.xscaler, self.yscaler\n",
    "#         else:\n",
    "        return self.xscaler\n",
    "    \n",
    "    # finding length of x \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    # indexing rows for calling\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79756b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_dataset = dataPrep1D(x_tensor, y_ko_tensor, batch_size = 16, scaler = MinMaxScaler, purpose = 'regression', test_split = 0.2)\n",
    "# reg_train_dataloader, reg_val_dataloader = reg_dataset.train_data()\n",
    "# reg_test_dataloader = reg_dataset.test_data()\n",
    "# reg_xscaler = reg_dataset.scaler('regression')\n",
    "\n",
    "regression_cycles = [100]\n",
    "# regression_cycles = [100]\n",
    "\n",
    "\n",
    "# x와 y가 1대1 대응이어야 dataloader를 만들 수 있어서 2D array 그대로 놓고 학습시 tensor transform 예정\n",
    "for reg in regression_cycles:\n",
    "    globals()[f'reg_dataset_{reg}'] = dataPrep_RNN_CNN(globals()[f'x_tensor_{reg}'], y_ko_tensor, batch_size = 4, scaler = MinMaxScaler, test_split = 0.2)\n",
    "    globals()[f'reg_train_dataloader_{reg}'], globals()[f'reg_val_dataloader_{reg}'] = globals()[f'reg_dataset_{reg}'].train_data()\n",
    "    globals()[f'reg_test_dataloader_{reg}'] = globals()[f'reg_dataset_{reg}'].test_data()\n",
    "    globals()[f'reg_xscaler_{reg}'] = globals()[f'reg_dataset_{reg}'].scaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f479e086",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = 100\n",
    "train_features, train_labels = next(iter(globals()[f'reg_train_dataloader_{reg}']))\n",
    "print(f'reg = {reg}')\n",
    "print(f\"Regression case_Inputs batch shape(Batch size, num_vars, timestep, cycles): {train_features.size()}\")\n",
    "print(f\"Regression case_Outputs batch shape(Batch size): {train_labels.size()}\")\n",
    "\n",
    "train_features.float().type()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313bef4c",
   "metadata": {},
   "source": [
    "## Define Model Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45fe621",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb81138",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNcell(nn.Module):\n",
    "    def __init__(self, model, input_size, hidden_size, dropout = 0.1, bidirectional = False, num_layer=1):\n",
    "        super(RNNcell, self).__init__()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "            \n",
    "        self.model = model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layer = num_layer\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.D = 1 + self.bidirectional\n",
    "        self.batch_size = 4\n",
    "        \n",
    "        #Batch size * cycles, timestep, input_size\n",
    "        if self.model.endswith('RNN'):\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layer, batch_first = True, dropout = dropout, bidirectional = bidirectional).to(self.device)\n",
    "        elif self.model.endswith('LSTM'):\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layer, batch_first = True, dropout = dropout, bidirectional = bidirectional).to(self.device)\n",
    "        elif self.model.endswith('GRU'):\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layer, batch_first = True, dropout = dropout, bidirectional = bidirectional).to(self.device)\n",
    "            \n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if name.startswith('weight'):\n",
    "                nn.init.xavier_uniform(param)\n",
    "            else:\n",
    "                nn.init.normal(param)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x made from (cell * n_cy, timestep(seq_len), num_vars)\n",
    "        self.batch_size = x.size(0)\n",
    "        \n",
    "        D = 1 + self.bidirectional\n",
    "        #out: Containing output features h_t from the last layer of LSTM for each t(if bidirectional ==True: concat of forward and backward hidden states)\n",
    "        #h_n: Containing final hidden state in the sequence\n",
    "        h0 = torch.zeros(D*self.num_layer, self.batch_size, self.hidden_size).to(self.device)\n",
    "        if self.model.endswith('LSTM'):\n",
    "            c0 = torch.zeros(D*self.num_layer, self.batch_size, self.hidden_size).to(self.device)\n",
    "            # c_n: Containing final cell state in the sequence\n",
    "            out, (hn, cn) = self.rnn(x, (h0, c0))\n",
    "        else:\n",
    "            out, hn = self.rnn(x, h0.detach())\n",
    "        return out, hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60895b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_TA_1DCNN(nn.Module):\n",
    "    def __init__(self, input_size, num_time, num_cycles, rnn1, bi1, hid1, fil2, pool2, npool2, fsize2, psize2, mids, dr1 = 0.1, dr2 = 0.1, di2 = 1, st2 = 1, pad2 = 0):\n",
    "        super(RNN_TA_1DCNN, self).__init__()\n",
    "        setRandomSeed()\n",
    "        \n",
    "        self.D1 = 1 + bi1\n",
    "        \n",
    "        self.rnn1 = RNNcell(rnn1, input_size, hid1, dr1, bi1)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "        \n",
    "        # Overall parameters\n",
    "        self.num_time = num_time\n",
    "        self.num_cycles = num_cycles\n",
    "        self.mids = mids\n",
    "        \n",
    "        # Pooling/Nonpooling parameters\n",
    "        self.pool2 = pool2\n",
    "        self.npool2 = npool2\n",
    "        # Pooling size(Pooling layer)\n",
    "        self.psize2 = psize2\n",
    "        \n",
    "        # CNN hyperparameters\n",
    "        self.fil2 = fil2\n",
    "        self.fsize2 = fsize2\n",
    "        self.di2 = di2\n",
    "        self.st2 = st2\n",
    "        # Padding for CNN\n",
    "        self.pad2 = pad2\n",
    "                \n",
    "        # Critical values for the 1st CNN layer\n",
    "        # Batch for different cells\n",
    "        \n",
    "        # Conv1d takes input dimension: [Batch_size, c_in(no. of in_channels )= hidden_size, l_in = num_cycles]\n",
    "        self.in2 = self.D1*hid1\n",
    "        self.out2 = self.fil2\n",
    "        self.Lout = self.num_cycles\n",
    "        \n",
    "        # Conv_block1~P\n",
    "        # Pooling layers\n",
    "        for i in range(1, self.pool2+1):\n",
    "#             print(f\"i: {i}, out_channels: {self.out_channels}\")\n",
    "            globals()[f'self.conv_block{i}'] = nn.Sequential(\n",
    "            nn.Conv1d(self.in2, self.out2, self.fsize2, self.st2, self.pad2),\n",
    "            nn.BatchNorm1d(self.out2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(self.psize2)\n",
    "            ).to(self.device)\n",
    "\n",
    "            self.in2 = self.out2\n",
    "            self.out2 = self.out2*2\n",
    "            self.Lout = int(int((self.Lout+2*self.pad2-self.di2*(self.fsize2-1)-1)/self.st2+1)/self.psize2)\n",
    "\n",
    "        for j in range(self.pool2+1, self.pool2+self.npool2+1):\n",
    "            globals()[f'self.conv_block{j}'] = nn.Sequential(\n",
    "            nn.Conv1d(self.in2, self.out2, self.fsize2, self.st2, self.pad2),\n",
    "            nn.BatchNorm1d(self.out2),\n",
    "            nn.ReLU(),\n",
    "            ).to(self.device)\n",
    "            \n",
    "            self.in2 = self.out2\n",
    "            self.out2 = self.out2*2\n",
    "            self.Lout = int((self.Lout+2*self.pad2-self.di2*(self.fsize2-1)-1)/self.st2+1)\n",
    "        \n",
    "        self.lin1 = nn.Linear(self.D1*hid1, 1)\n",
    "        self.sm1 = nn.Softmax(dim = 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Linear(self.in2*self.Lout, self.mids[0], bias=True), nn.ReLU(),\n",
    "                       nn.Linear(self.mids[0], self.mids[1], bias = True), nn.ReLU(),\n",
    "                       nn.Linear(self.mids[1], self.mids[2], bias = True))\n",
    "        \n",
    "        self.fc.apply(self.init_weights)\n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        self.m = m\n",
    "        if type(self.m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(self.m.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Original: Batch size, num_vars, timestep, cycles=>Batch size * cycles, timestep, input_size\n",
    "        x = torch.reshape(x, (x.size(0)*x.size(3), x.size(2), x.size(1))).to(device)\n",
    "#         print(\"Batch size * cycles, timestep, input_size: \", x.size())\n",
    "        out1, hn1 = self.rnn1(x)\n",
    "        \n",
    "        # hn의 batch size가 n_cy*k로 설정\n",
    "        cell_batch_size = int(hn1.size(1)/self.num_cycles)\n",
    "\n",
    "        # Reshape for input\n",
    "        # out1: (batch_size = num_cell*num_cycle, timestep, D1*hid1)\n",
    "        # outs : (batch_size(num_cell), num_cycle, timestep, D1*hid1)\n",
    "        outs1 = torch.reshape(out1, (cell_batch_size, -1, out1.size(1), out1.size(2)))\n",
    "        # ta: (batch_size, num_cycle, timestep, 1(D1*hid1->1))\n",
    "        ta = self.sm1(self.lin1(outs1))\n",
    "        # ta_outs: (batch_size, num_cycle, timestep, D1*hid1)=>(batch_size, D1*hid1, num_cycle, timestep)\n",
    "        ta_outs = torch.permute(ta*outs1, (0, 3, 1, 2))\n",
    "        # ct_vec: (batch_size, D1*hid1, num_cycle)\n",
    "        ct_vec = torch.sum(ta_outs, -1)\n",
    "        \n",
    "        # input for 1d cnn: (batch_size, D1*hid1, num_cycle)\n",
    "        for i in range(1, self.pool2+self.npool2+1):\n",
    "            ct_vec = globals()[f'self.conv_block{i}'](ct_vec)\n",
    "        # ct_vec from final convolutional layer: (batch_size, self.in2*self.Lout)\n",
    "        ct_vec = ct_vec.view(ct_vec.size(0), -1)\n",
    "        # final_out: (batch_size, 1)\n",
    "        final_out = self.fc(ct_vec)\n",
    "        \n",
    "        return final_out.squeeze(), ta.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f4a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for saving and loading of training history\n",
    "def save_data(D3_array, filename):\n",
    "    with open(filename,\"wb\") as dat_:\n",
    "        pickle.dump(D3_array,dat_)\n",
    "        \n",
    "def load_data(filename):\n",
    "    with open(filename,\"rb\") as ld:\n",
    "        x_temp = pickle.load(ld)\n",
    "    return x_temp\n",
    "\n",
    "# Early stopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.best_epoch = 1\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, epoch, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model = self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "#             print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            self.best_model = self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'At epoch {self.best_epoch}: Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac79d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class train:\n",
    "    def __init__(self, model: nn.Module, train_dataloader, val_dataloader, test_dataloader, epoch: int, learning_rate=0.01, patience = 5, verbose = False):\n",
    "        super().__init__()\n",
    "        #   Reprocudtion\n",
    "        setRandomSeed(100)\n",
    "        random_seed = 100\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(random_seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "        os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "        torch.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "        \n",
    "        self.model = model.to(self.device)\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.epoch = epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr = self.learning_rate)\n",
    "        self.early_stopping = EarlyStopping(patience = patience, verbose = verbose)\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.name = ['train_rmse', 'val_rmse', 'test_rmse']\n",
    "        \n",
    "    def EvalModel(self):\n",
    "        EvalPredictions, EvalActuals = list(), list()\n",
    "        TestPredictions, TestActuals = list(), list()\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for n, (inputs, targets) in enumerate(self.val_dataloader):\n",
    "                inputs = inputs.float().to(self.device)\n",
    "                targets = targets.float().to(self.device)\n",
    "\n",
    "                yhat, ta = self.model(inputs)\n",
    "\n",
    "                if self.device !='cpu':\n",
    "                    yhat = yhat.to('cpu')\n",
    "                    targets = targets.to('cpu')    \n",
    "\n",
    "                yhat = yhat.detach().numpy()\n",
    "                actual = targets.numpy()\n",
    "                EvalPredictions.append(yhat)\n",
    "                EvalActuals.append(actual)\n",
    "\n",
    "            EvalPredictions, EvalActuals = vstack(EvalPredictions), vstack(EvalActuals)\n",
    "            val_rmse = mean_squared_error(EvalActuals, EvalPredictions, squared = False)\n",
    "            \n",
    "            for n, (inputs, targets) in enumerate(self.test_dataloader):\n",
    "                inputs = inputs.float().to(self.device)\n",
    "                targets = targets.float().to(self.device)\n",
    "\n",
    "                yhat, ta = self.model(inputs)\n",
    "\n",
    "                if self.device !='cpu':\n",
    "                    yhat = yhat.to('cpu')\n",
    "                    targets = targets.to('cpu')    \n",
    "\n",
    "                yhat = yhat.detach().numpy()\n",
    "                actual = targets.numpy()\n",
    "                TestPredictions.append(yhat)\n",
    "                TestActuals.append(actual)\n",
    "\n",
    "            TestPredictions, TestActuals = vstack(TestPredictions), vstack(TestActuals)\n",
    "            test_rmse = mean_squared_error(TestActuals, TestPredictions, squared = False)\n",
    "            \n",
    "            return val_rmse, test_rmse\n",
    "\n",
    "    def TrainModel(self):\n",
    "        loss_history = []\n",
    "        self.model.train()\n",
    "        for i in range(self.epoch):\n",
    "            predictions, actuals = list(), list()\n",
    "            for n, (inputs, targets) in enumerate(self.train_dataloader):\n",
    "                self.model.train()\n",
    "                inputs = inputs.float().to(self.device)\n",
    "                targets = targets.float().to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                yhat, ta = self.model(inputs)\n",
    "                    \n",
    "                loss = self.criterion(yhat, targets)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                if self.device !='cpu':\n",
    "                    yhat = yhat.to('cpu')\n",
    "                    targets = targets.to('cpu')\n",
    "                    \n",
    "                yhat = yhat.detach().numpy()\n",
    "                predictions.append(yhat)\n",
    "                \n",
    "                targets = targets.numpy()\n",
    "                actuals.append(targets)\n",
    "\n",
    "            # evaluation of entire train data\n",
    "            predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "            \n",
    "            train_rmse = mean_squared_error(actuals, predictions, squared = False)\n",
    "\n",
    "            # evaluation on validation data\n",
    "            val_rmse, test_rmse = self.EvalModel()\n",
    "            early_factor = val_rmse\n",
    "            \n",
    "            # training loss history\n",
    "            loss_history.append([train_rmse, val_rmse, test_rmse])\n",
    "\n",
    "            self.early_stopping(i+1, early_factor, self.model)\n",
    "            \n",
    "            if self.early_stopping.early_stop:\n",
    "                print(\"Early stopping at best epoch: \", self.early_stopping.best_epoch)\n",
    "                self.best_loss = loss_history[self.early_stopping.best_epoch-1]\n",
    "                loss_history = loss_history[:self.early_stopping.best_epoch]\n",
    "                \n",
    "                return self.model, self.best_loss, pd.DataFrame(loss_history, columns = self.name), ta\n",
    "            \n",
    "        print(f\"Ends at final epoch {self.epoch}\")\n",
    "        print(f\"Best epoch: {self.early_stopping.best_epoch}\")\n",
    "        self.best_loss = loss_history[self.early_stopping.best_epoch-1]\n",
    "        loss_history = loss_history[:self.early_stopping.best_epoch]\n",
    "        \n",
    "        return self.model, self.best_loss, pd.DataFrame(loss_history, columns = self.name), ta\n",
    "        \n",
    "    def predict(self, data: torch.Tensor):\n",
    "        return self.model(data)\n",
    "    \n",
    "    def train_val_test_plot_rmse(self, num_cycles, modelname, n_ep, patience, rnn1, hid1, fil2, pool2, npool2, fsize2, psize2, lr):\n",
    "        dataloaders = [self.train_dataloader, self.val_dataloader, self.test_dataloader]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            model_rmse = list()\n",
    "            dset_index = 0\n",
    "            for dataloader in dataloaders:\n",
    "                if dset_index ==0:\n",
    "                    dset = 'train'\n",
    "                elif dset_index == 1:\n",
    "                    dset = 'val'\n",
    "                else:\n",
    "                    dset = 'test'\n",
    "\n",
    "                figadd = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, f'{rnn1}_1D CNN', 'train_history'], \n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}.jpg')\n",
    "\n",
    "                predictions, actuals = list(), list()\n",
    "                for i, (test_input, test_target) in enumerate(dataloader):\n",
    "                    test_input = test_input.float().to(self.device)\n",
    "                    test_target = test_target.float().to(self.device)\n",
    "                    test_yhat, test_ta = model(test_input)\n",
    "\n",
    "                    if self.device != 'cpu':\n",
    "                        test_yhat = test_yhat.to('cpu')\n",
    "                        test_target = test_target.to('cpu')\n",
    "\n",
    "                    test_yhat = test_yhat.detach().tolist()\n",
    "                    predictions += test_yhat\n",
    "                    test_target = test_target.tolist()\n",
    "                    actuals += test_target\n",
    "\n",
    "                pred_acc, targ_acc = np.array(predictions), np.array(actuals)\n",
    "\n",
    "                rmse = mean_squared_error(actuals, predictions, squared = False)\n",
    "                model_rmse.append(rmse)\n",
    "\n",
    "                plt.figure(figsize=[10,4])\n",
    "                plt.rcParams[\"font.size\"] = \"12\"\n",
    "                plt.plot(pred_acc, label='Prediction')\n",
    "                plt.plot(range(len(targ_acc)), targ_acc, label = 'Actual')\n",
    "                plt.xticks(range(0, len(targ_acc) + 1, int(len(targ_acc)/5)))\n",
    "                plt.title(f\"{rnn1}_hid_{hid1}_nfil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}: RMSE={rmse}\")\n",
    "                plt.xlabel('cell')\n",
    "                plt.ylabel('Lifespan(cycle)')\n",
    "                plt.legend()\n",
    "                plt.savefig(figadd)\n",
    "                plt.close()\n",
    "\n",
    "                dset_index += 1\n",
    "\n",
    "        return model_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5910dbff",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_state_dict_add(num_cycles, modelname, n_ep, patience, rnn1, hid1, fil2, pool2, npool2, fsize2, psize2, lr):\n",
    "    history_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, f'{rnn1}_1D CNN', 'train_history'], \n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}.pkl')\n",
    "    state_dict_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, f'{rnn1}_1D CNN', 'model'], \n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}_state_dict.pth')\n",
    "    ta_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, f'{rnn1}_1D CNN', 'ta'],\n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}_ta.wb')\n",
    "    return history_add, state_dict_add, ta_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_vars = train_features.size()[1]\n",
    "\n",
    "modelname = 'RNN_TA_1DCNN'\n",
    "# rnns = ['LSTM', 'GRU', 'RNN']\n",
    "rnns = ['RNN']\n",
    "hids = [3, 5, 7]\n",
    "num_time = 1000\n",
    "num_cycles = 100\n",
    "# ep_pats = [[1000, 10], [2000, 20], [3000, 500]]\n",
    "ep_pats = [[3000, 500]]\n",
    "lrs = [1e-4, 1e-3, 1e-2]\n",
    "\n",
    "num_fils = [3, 5, 7]\n",
    "pools = [1, 2]\n",
    "npools = [1, 2]\n",
    "fsize2 = 3\n",
    "psize2 = 2\n",
    "mids = [8, 4, 1]\n",
    "\n",
    "import time\n",
    "\n",
    "for [n_ep, patience] in ep_pats:\n",
    "    print(f\"epoch = {n_ep}, patience = {patience}\")\n",
    "    trdl = globals()[f'reg_train_dataloader_{num_cycles}']\n",
    "    vdl = globals()[f'reg_val_dataloader_{num_cycles}']\n",
    "    tedl = globals()[f'reg_test_dataloader_{num_cycles}']\n",
    "    for rnn1, hid1 in itertools.product(rnns, hids):\n",
    "        bi1 = False\n",
    "        if rnn1.startswith('Bi'):\n",
    "            bi1 = True\n",
    "        for pool2, npool2 in itertools.product(pools, npools):\n",
    "            print(f'Discharging dataset, Pooling: {pool2} layers, Nonpooling: {npool2} layers')\n",
    "            for fil2, lr in itertools.product(num_fils, lrs):\n",
    "                if rnn1 == 'GRU' and hid1 ==3 and pool2 == 1 and npool2 == 2 and hid1 == 3 and fil2 == 7 and lr == 0.01:\n",
    "                    pass\n",
    "                else:\n",
    "                    start = time.time()\n",
    "                    print(f\"rnn = {rnn1}, bi = {bi1}, hid = {hid1}, num_fil = {fil2}, lr = {lr}\")\n",
    "\n",
    "                    # Construct CNN\n",
    "                    model = globals()[modelname](num_vars, num_time, num_cycles, rnn1, bi1, hid1, fil2, pool2, npool2, fsize2, psize2, mids).to(device)\n",
    "\n",
    "                    # Train\n",
    "                    model_train = train(model, trdl, vdl, tedl, n_ep, lr, patience, verbose = False)\n",
    "                    best_model, best_loss, history, ta = model_train.TrainModel()\n",
    "\n",
    "                    print('best_loss = '+ str(best_loss))\n",
    "\n",
    "                    history_add, state_dict_add, ta_add = history_state_dict_add(num_cycles, modelname, n_ep, patience,\n",
    "                                                                                rnn1, hid1, fil2, pool2, npool2, fsize2, psize2, lr)\n",
    "\n",
    "                    # saving best_epoch, loss history, ta score\n",
    "                    save_data(history, history_add)\n",
    "                    save_data(best_model.state_dict(), state_dict_add)\n",
    "                    save_data(ta, ta_add)\n",
    "\n",
    "                    print(\"time: \", time.time()-start)\n",
    "                    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c83b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_vars = train_features.size()[1]\n",
    "\n",
    "modelname = 'RNN_TA_1DCNN'\n",
    "# rnns = ['LSTM', 'GRU', 'RNN']\n",
    "rnns = ['GRU']\n",
    "hids = [5, 7]\n",
    "num_time = 1000\n",
    "num_cycles = 100\n",
    "# ep_pats = [[1000, 10], [2000, 20], [3000, 500]]\n",
    "ep_pats = [[3000, 500]]\n",
    "lrs = [1e-4, 1e-3, 1e-2]\n",
    "\n",
    "num_fils = [3, 5, 7]\n",
    "pools = [1, 2]\n",
    "npools = [1, 2]\n",
    "fsize2 = 3\n",
    "psize2 = 2\n",
    "mids = [8, 4, 1]\n",
    "\n",
    "import time\n",
    "\n",
    "for [n_ep, patience] in ep_pats:\n",
    "    print(f\"epoch = {n_ep}, patience = {patience}\")\n",
    "    trdl = globals()[f'reg_train_dataloader_{num_cycles}']\n",
    "    vdl = globals()[f'reg_val_dataloader_{num_cycles}']\n",
    "    tedl = globals()[f'reg_test_dataloader_{num_cycles}']\n",
    "    for rnn1, hid1 in itertools.product(rnns, hids):\n",
    "        bi1 = False\n",
    "        if rnn1.startswith('Bi'):\n",
    "            bi1 = True\n",
    "        for pool2, npool2 in itertools.product(pools, npools):\n",
    "            print(f'Discharging dataset, Pooling: {pool2} layers, Nonpooling: {npool2} layers')\n",
    "            for fil2, lr in itertools.product(num_fils, lrs):\n",
    "                if pool2 == 1 and npool2 ==1:\n",
    "                    pass\n",
    "                elif rnn1 == 'GRU' and hid1 == 3 and pool2 == 1 and npool2 == 2 and fil2 == 7 and lr == 0.01:\n",
    "                    pass\n",
    "                else:\n",
    "                    start = time.time()\n",
    "                    print(f\"rnn = {rnn1}, bi = {bi1}, hid = {hid1}, num_fil = {fil2}, lr = {lr}\")\n",
    "\n",
    "                    # Construct CNN\n",
    "                    model = globals()[modelname](num_vars, num_time, num_cycles, rnn1, bi1, hid1, fil2, pool2, npool2, fsize2, psize2, mids).to(device)\n",
    "\n",
    "                    # Train\n",
    "                    model_train = train(model, trdl, vdl, tedl, n_ep, lr, patience, verbose = False)\n",
    "                    best_model, best_loss, history, ta = model_train.TrainModel()\n",
    "\n",
    "                    print('best_loss = '+ str(best_loss))\n",
    "\n",
    "                    history_add, state_dict_add, ta_add = history_state_dict_add(num_cycles, modelname, n_ep, patience,\n",
    "                                                                                rnn1, hid1, fil2, pool2, npool2, fsize2, psize2, lr)\n",
    "\n",
    "                    # saving best_epoch, loss history, ta score\n",
    "                    save_data(history, history_add)\n",
    "                    save_data(best_model.state_dict(), state_dict_add)\n",
    "                    save_data(ta, ta_add)\n",
    "\n",
    "                    print(\"time: \", time.time()-start)\n",
    "                    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b939a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "modelname = 'RNN_TA_1DCNN'\n",
    "rnns = ['LSTM', 'GRU', 'RNN']\n",
    "hids = [3, 5, 7]\n",
    "num_time = 120\n",
    "num_cycles = 100\n",
    "ep_pats = [[1000, 10], [2000, 20], [3000, 500]]\n",
    "lrs = [1e-4, 1e-3, 1e-2]\n",
    "\n",
    "num_fils = [3, 5, 7]\n",
    "pools = [1, 2]\n",
    "npools = [1, 2]\n",
    "fsize2 = 3\n",
    "psize2 = 2\n",
    "mids = [8, 4, 1]\n",
    "\n",
    "for [n_ep, patience] in ep_pats:\n",
    "    for rnn1 in rnns:\n",
    "        \n",
    "        df = pd.DataFrame(columns = ['train rmse', 'val rmse', 'test rmse'])\n",
    "        bi1 = False\n",
    "        if rnn1.startswith('Bi'):\n",
    "            bi1 = True\n",
    "        for hid1 in hids:\n",
    "            for pool2, npool2 in itertools.product(pools, npools):\n",
    "                for fil2, lr in itertools.product(num_fils, lrs):\n",
    "                    history_add, _, _ = history_state_dict_add(num_cycles, modelname, n_ep, patience, rnn1, hid1, fil2, pool2, npool2, fsize2, psize2, lr)\n",
    "                    history = load_data(history_add)\n",
    "\n",
    "                    df.loc[f'{rnn1}_{hid1}_{fil2}_{pool2}_{npool2}_{fsize2}_{lr}'] = [history.iloc[-1, 0], history.iloc[-1, 1], history.iloc[-1, 2]]\n",
    "\n",
    "        print(n_ep, patience, rnn1)\n",
    "        df\n",
    "        df_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname], \n",
    "                        filename = f'{modelname}_{rnn1}.csv')\n",
    "        df.to_csv(df_add)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
