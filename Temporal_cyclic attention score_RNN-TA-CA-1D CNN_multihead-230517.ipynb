{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14c8de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import pytorchtools\n",
    "import glob as gl\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.datasets import make_regression\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import log_loss\n",
    "from numpy import hstack\n",
    "from numpy import vstack\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6062fb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\admin\\\\Desktop\\\\LIB\\\\CNN work\\\\Variable time length\\\\All dataset'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Desktop\\LIB\\CNN work\\Variable time length\\Dataset created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\admin\\\\Desktop\\\\LIB\\\\CNN work\\\\Variable time length\\\\Dataset\\\\bat_dict.pkl'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_directory(foldername, filename = None, back_num = 0):\n",
    "    cur = os.getcwd()\n",
    "    for i in range(back_num):\n",
    "        cur = os.path.abspath(os.path.join(cur, os.pardir))\n",
    "    for folder in foldername:\n",
    "        cur = os.path.join(cur, folder)\n",
    "    if not os.path.exists(cur):\n",
    "        os.makedirs(cur)\n",
    "        print(f'{cur} created')\n",
    "    if filename != None:\n",
    "        cur = os.path.join(cur, filename)\n",
    "    return cur\n",
    "\n",
    "os.getcwd()\n",
    "find_directory(back_num = 1, foldername = ['Dataset'], filename = 'bat_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bc895e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bat_dict_add_1_2 = find_directory(back_num = 2, foldername = ['Dataset'], filename = 'bat_1_2_dict_VITQcQd.pkl')\n",
    "with open(bat_dict_add_1_2, 'rb') as tf:\n",
    "    bat_dict_1_2 = pickle.load(tf)\n",
    "\n",
    "bat_sel_dict = bat_dict_1_2\n",
    "max_time = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38245352",
   "metadata": {},
   "source": [
    "## Split per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6794aabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 43 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([124, 100, 120, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch1: 0~40(41)\n",
    "# batch2: 41~83(43)\n",
    "# batch3: 64~123(40)\n",
    "\n",
    "b1 = 0\n",
    "b2 = 0\n",
    "b3 = 0\n",
    "\n",
    "for cell in bat_sel_dict.keys():\n",
    "    if cell.startswith('b1'):\n",
    "        b1 += 1\n",
    "    elif cell.startswith('b2'):\n",
    "        b2 += 1\n",
    "    else:\n",
    "        b3 += 1\n",
    "\n",
    "print(b1, b2, b3)\n",
    "\n",
    "# Tensor dimension: (cell, cycle, time, variable)\n",
    "regression_cycles = [100]\n",
    "\n",
    "# variable = [Qc, I, V, T, Qd]\n",
    "num_variables = len(bat_sel_dict['b1c0']['1']['1.0'].keys())\n",
    "\n",
    "for reg in regression_cycles:\n",
    "    globals()[f'x_tensor_{reg}'] = torch.zeros(b1+b2+b3, reg, max_time, num_variables)\n",
    "    cell_index = 1\n",
    "    for cell in bat_sel_dict.keys():\n",
    "        for cycle in range(1, reg+1):\n",
    "            for time in bat_sel_dict[cell][str(cycle)].keys():\n",
    "    #             print(bat_sel_dict[cell][str(cycle)][str(time)])\n",
    "                value_list = [i for i in bat_sel_dict[cell][str(cycle)][str(time)].values()]\n",
    "    #             print(value_list)\n",
    "                globals()[f'x_tensor_{reg}'][cell_index-1, cycle-1, int(float(time)*max_time/60), :] = torch.FloatTensor(value_list)\n",
    "\n",
    "        cell_index += 1\n",
    "    \n",
    "    globals()[f'x_tensor_{reg}'].size()\n",
    "\n",
    "# globals()[f'x_tensor_b1_{reg}'] = globals()[f'x_tensor_{reg}'][:b1, :, :, :]\n",
    "# globals()[f'x_tensor_b2_{reg}'] = globals()[f'x_tensor_{reg}'][b1:b1+b2, :, :, :]\n",
    "# globals()[f'x_tensor_b3_{reg}'] = globals()[f'x_tensor_{reg}'][b1+b2:, :, :, :]\n",
    "    \n",
    "# globals()[f'x_tensor_b1_{reg}'].size()\n",
    "# globals()[f'x_tensor_b2_{reg}'].size()\n",
    "# globals()[f'x_tensor_b3_{reg}'].size()\n",
    "\n",
    "num_variables = x_tensor_100.shape[3]\n",
    "time_length = x_tensor_100.shape[2]\n",
    "# (cycle_index-1, cycle-1, time-1, variable(Qc, I, V, T, Qd))\n",
    "# x_tensor[123, 99, 30, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d1e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df_add = find_directory(back_num = 2, foldername = ['Dataset', 'y_df.csv'])\n",
    "y_df = pd.read_csv(y_df_add)\n",
    "\n",
    "# 2. Regression problem\n",
    "# KO, KP, CL\n",
    "y_cl_numpy = y_df['Cycle life'].to_numpy()\n",
    "y_kp_numpy = y_df['kneepoints'].to_numpy()\n",
    "y_ko_numpy = y_df['kneeonsets'].to_numpy()\n",
    "# y_ko_numpy.shape\n",
    "\n",
    "y_regression_numpy = y_ko_numpy\n",
    "\n",
    "y_ko_tensor = torch.FloatTensor(y_ko_numpy)\n",
    "# y_ko_tensor.size()\n",
    "\n",
    "# y_ko_b1_tensor = y_ko_tensor[:b1]\n",
    "# y_ko_b2_tensor = y_ko_tensor[b1:b1+b2]\n",
    "# y_ko_b3_tensor = y_ko_tensor[b1+b2:]\n",
    "# y_ko_b1_tensor.size()\n",
    "# y_ko_b2_tensor.size()\n",
    "# y_ko_b3_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setRandomSeed(random_seed=0):\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "    torch.manual_seed(random_seed) # torch \n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True # cudnn\n",
    "    torch.backends.cudnn.benchmark = False # cudnn\n",
    "    np.random.seed(random_seed) # numpy\n",
    "    random.seed(random_seed) # random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d34896f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# y: no scaling\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mdataPrep_RNN_CNN\u001b[39;00m(\u001b[43mDataset\u001b[49m):  \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_tensor, y_tensor, batch_size, scaler, b1, b2, b3):\n\u001b[0;32m      4\u001b[0m         setRandomSeed(random_seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# y: no scaling\n",
    "class dataPrep_RNN_CNN(Dataset):  \n",
    "    def __init__(self, x_tensor, y_tensor, batch_size, scaler, b1, b2, b3):\n",
    "        setRandomSeed(random_seed = 100)\n",
    "        self.xdata = torch.permute(x_tensor, (0, 3, 2, 1))\n",
    "        self.ydata = y_tensor\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "                \n",
    "        self.xscaler = scaler()\n",
    "        \n",
    "        # Tensor dimension: (cell, variable, time, cycle)\n",
    "        # If problem emerges, check this part first.\n",
    "        for i in range(len(self.xdata)):\n",
    "            for j in range(self.xdata.shape[1]):\n",
    "                temp = np.expand_dims(self.xscaler.fit_transform(self.xdata[i,j,:,:]), axis=0)\n",
    "                if j ==0:\n",
    "                    temp2 = temp\n",
    "                else:\n",
    "                    temp2 = np.vstack((temp2,temp))\n",
    "            temp2 = np.expand_dims(temp2, axis=0)\n",
    "            if i==0:\n",
    "                self.xdata_scaled = temp2\n",
    "            else:\n",
    "                self.xdata_scaled = np.vstack((self.xdata_scaled, temp2))\n",
    "\n",
    "        self.ydata_scaled = self.ydata\n",
    "        \n",
    "        self.all = list(zip(self.xdata_scaled, self.ydata_scaled))\n",
    "        \n",
    "        self.b1 = self.all[:b1]\n",
    "        self.b2 = self.all[b1:b1+b2]\n",
    "        self.b3 = self.all[b1+b2:]\n",
    "        \n",
    "        self.b1_dataloader = DataLoader(self.b1, batch_size = self.batch_size, shuffle = True)\n",
    "        self.b2_dataloader = DataLoader(self.b2, batch_size = self.batch_size, shuffle = True)\n",
    "        self.b3_dataloader = DataLoader(self.b3, batch_size = self.batch_size, shuffle = True)\n",
    "        \n",
    "        print(f\"batch 1: {len(self.b1)}, batch 2: {len(self.b2)}, batch 3: {len(self.b3)}\")\n",
    "    \n",
    "    def batch_dataloader(self):\n",
    "        return self.b1_dataloader, self.b2_dataloader, self.b3_dataloader\n",
    "    \n",
    "    def scaler(self):\n",
    "        return self.xscaler\n",
    "    \n",
    "    # finding length of x \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    # indexing rows for calling\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d65f116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1: 41, batch 2: 43, batch 3: 40\n"
     ]
    }
   ],
   "source": [
    "regression_cycles = [100]\n",
    "\n",
    "\n",
    "# x와 y가 1대1 대응이어야 dataloader를 만들 수 있어서 2D array 그대로 놓고 학습시 tensor transform 예정\n",
    "for reg in regression_cycles:\n",
    "    globals()[f'reg_dataset_{reg}'] = dataPrep_RNN_CNN(globals()[f'x_tensor_{reg}'], y_ko_tensor, 4, MinMaxScaler, b1, b2, b3)\n",
    "    globals()[f'b1_dataloader_{reg}'], globals()[f'b2_dataloader_{reg}'], globals()[f'b3_dataloader_{reg}'] = globals()[f'reg_dataset_{reg}'].batch_dataloader()\n",
    "    globals()[f'reg_xscaler_{reg}'] = globals()[f'reg_dataset_{reg}'].scaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2380a1",
   "metadata": {},
   "source": [
    "## Define model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07e2dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNcell(nn.Module):\n",
    "    def __init__(self, model, input_size, hidden_size, dropout = 0.1, bidirectional = False, num_layer=1):\n",
    "        super(RNNcell, self).__init__()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "            \n",
    "        self.model = model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layer = num_layer\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.D = 1 + self.bidirectional\n",
    "        self.batch_size = 4\n",
    "        \n",
    "        #Batch size * cycles, timestep, input_size\n",
    "        if self.model.endswith('RNN'):\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layer, batch_first = True, dropout = dropout, bidirectional = bidirectional).to(self.device)\n",
    "        elif self.model.endswith('LSTM'):\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layer, batch_first = True, dropout = dropout, bidirectional = bidirectional).to(self.device)\n",
    "        elif self.model.endswith('GRU'):\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layer, batch_first = True, dropout = dropout, bidirectional = bidirectional).to(self.device)\n",
    "            \n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if name.startswith('weight'):\n",
    "                nn.init.xavier_uniform(param)\n",
    "            else:\n",
    "                nn.init.normal(param)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x made from (cell * n_cy, timestep(seq_len), num_vars)\n",
    "        self.batch_size = x.size(0)\n",
    "        \n",
    "        D = 1 + self.bidirectional\n",
    "        #out: Containing output features h_t from the last layer of LSTM for each t(if bidirectional ==True: concat of forward and backward hidden states)\n",
    "        #h_n: Containing final hidden state in the sequence\n",
    "        h0 = torch.zeros(D*self.num_layer, self.batch_size, self.hidden_size).to(self.device)\n",
    "        if self.model.endswith('LSTM'):\n",
    "            c0 = torch.zeros(D*self.num_layer, self.batch_size, self.hidden_size).to(self.device)\n",
    "            # c_n: Containing final cell state in the sequence\n",
    "            out, (hn, cn) = self.rnn(x, (h0, c0))\n",
    "        else:\n",
    "            out, hn = self.rnn(x, h0.detach())\n",
    "        return out, hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "763d0d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_size // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # inputs shape: [batch_size, seq_len, hidden_size]\n",
    "        batch_size, seq_len, hidden_size = inputs.size()\n",
    "        \n",
    "        # Compute query, key, and value matrices\n",
    "        query = self.query(inputs).view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        key = self.key(inputs).view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        value = self.value(inputs).view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        \n",
    "        # Compute scaled dot-product attention scores\n",
    "        # attention_scores: [batch_size, num_heads, seq_len, seq_len]\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_size, dtype=torch.float32))\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        \n",
    "        # Apply attention to the value matrix and concatenate heads\n",
    "        # context: [batch_size, num_heads, seq_len, head_size]=>[batch_size, seq_len, num_heads, head_size] \n",
    "        # => [batch_size, seq_len, num_heads * head_size]\n",
    "        context = torch.matmul(attention_probs, value).transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "#         context = nn.Linear(hidden_size, 1)(context)\n",
    "        return attention_probs, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d388a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_TA_CA_1DCNN(nn.Module):\n",
    "    def __init__(self, input_size, num_time, num_cycles, rnn1, bi1, hid1, nhead2, fil2, pool2, npool2, fsize2, psize2, mids, dr1 = 0.1, dr2 = 0.1, di2 = 1, st2 = 1, pad2 = 0):\n",
    "        super(RNN_TA_CA_1DCNN, self).__init__()\n",
    "        setRandomSeed()\n",
    "        \n",
    "        self.D1 = 1 + bi1\n",
    "        \n",
    "        self.rnn1 = RNNcell(rnn1, input_size, hid1, dr1, bi1)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "        \n",
    "        # Overall parameters\n",
    "        self.num_time = num_time\n",
    "        self.num_cycles = num_cycles\n",
    "        self.mids = mids\n",
    "        \n",
    "        # Pooling/Nonpooling parameters\n",
    "        # Pooling/Nonpooling layers\n",
    "        self.pool2 = pool2\n",
    "        self.npool2 = npool2\n",
    "        # Pooling size(Pooling layer)\n",
    "        self.psize2 = psize2\n",
    "        \n",
    "        # Params for self-attention(ca)\n",
    "        self.nh2 = nhead2\n",
    "        \n",
    "        # CNN hyperparameters\n",
    "        self.fil2 = fil2\n",
    "        self.fsize2 = fsize2\n",
    "        self.di2 = di2\n",
    "        self.st2 = st2\n",
    "        # Padding for CNN\n",
    "        self.pad2 = pad2\n",
    "                \n",
    "        # Critical values for the 1st CNN layer\n",
    "        # Batch for different cells\n",
    "        \n",
    "        # Conv1d takes input dimension: [Batch_size, c_in(no. of in_channels )= hidden_size, l_in = num_cycles]\n",
    "        self.in2 = self.D1*hid1\n",
    "        self.out2 = self.fil2\n",
    "        self.Lout = self.num_cycles\n",
    "        \n",
    "        # Conv_block1~P\n",
    "        # Pooling layers\n",
    "        for i in range(1, self.pool2+1):\n",
    "#             print(f\"i: {i}, out_channels: {self.out_channels}\")\n",
    "            globals()[f'self.conv_block{i}'] = nn.Sequential(\n",
    "            nn.Conv1d(self.in2, self.out2, self.fsize2, self.st2, self.pad2),\n",
    "            nn.BatchNorm1d(self.out2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(self.psize2)\n",
    "            ).to(self.device)\n",
    "\n",
    "            self.in2 = self.out2\n",
    "            self.out2 = self.out2*2\n",
    "            self.Lout = int(int((self.Lout+2*self.pad2-self.di2*(self.fsize2-1)-1)/self.st2+1)/self.psize2)\n",
    "\n",
    "        for j in range(self.pool2+1, self.pool2+self.npool2+1):\n",
    "            globals()[f'self.conv_block{j}'] = nn.Sequential(\n",
    "            nn.Conv1d(self.in2, self.out2, self.fsize2, self.st2, self.pad2),\n",
    "            nn.BatchNorm1d(self.out2),\n",
    "            nn.ReLU(),\n",
    "            ).to(self.device)\n",
    "            \n",
    "            self.in2 = self.out2\n",
    "            self.out2 = self.out2*2\n",
    "            self.Lout = int((self.Lout+2*self.pad2-self.di2*(self.fsize2-1)-1)/self.st2+1)\n",
    "        \n",
    "        self.lin1 = nn.Linear(self.D1*hid1, 1)\n",
    "        self.sm1 = nn.Softmax(dim = 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.ca = SelfAttention(self.D1*hid1, self.nh2)\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Linear(self.in2*self.Lout, self.mids[0], bias=True), nn.ReLU(),\n",
    "                       nn.Linear(self.mids[0], self.mids[1], bias = True), nn.ReLU(),\n",
    "                       nn.Linear(self.mids[1], self.mids[2], bias = True))\n",
    "        \n",
    "        self.fc.apply(self.init_weights)\n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        self.m = m\n",
    "        if type(self.m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(self.m.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Original: Batch size, num_vars, timestep, cycles=>Batch size * cycles, timestep, input_size\n",
    "        x = torch.reshape(x, (x.size(0)*x.size(3), x.size(2), x.size(1))).to(device)\n",
    "#         print(\"Batch size * cycles, timestep, input_size: \", x.size())\n",
    "        out1, hn1 = self.rnn1(x)\n",
    "        \n",
    "        # hn의 batch size가 n_cy*k로 설정\n",
    "        cell_batch_size = int(hn1.size(1)/self.num_cycles)\n",
    "\n",
    "        # Reshape for input\n",
    "        # out1: (batch_size = num_cell*num_cycle, timestep, D1*hid1)\n",
    "        # outs : (batch_size(num_cell), num_cycle, timestep, D1*hid1)\n",
    "        outs1 = torch.reshape(out1, (cell_batch_size, -1, out1.size(1), out1.size(2)))\n",
    "        # ta: (batch_size, num_cycle, timestep, 1(D1*hid1->1))\n",
    "        ta = self.sm1(self.lin1(outs1))\n",
    "        # ta_outs: (batch_size, num_cycle, timestep, D1*hid1)=>(batch_size, D1*hid1, num_cycle, timestep)\n",
    "        ta_outs = torch.permute(ta*outs1, (0, 3, 1, 2))\n",
    "        # ct_vec: (batch_size, D1*hid1, num_cycle)\n",
    "        ct_vec = torch.sum(ta_outs, -1)\n",
    "        # ct_vec shape: [batch_size, num_cycle, D1*hid1(hidden_size)]\n",
    "        # hidden_size = num_heads * head_size\n",
    "        # ct_vec transform back: [batch_size, num_heads, num_cycle, head_size]=>[batch_size, num_cycle, num_heads, head_size] \n",
    "        # => [batch_size, num_cycle, num_heads * head_size(hidden_size)]\n",
    "        # ca: matmul(query[batch_size, num_heads, num_cycle, head_size], key^T[batch_size, num_heads, head_size, num_cycle])\n",
    "        # ca shape: [batch_size, num_heads, num_cycle, num_cycle]\n",
    "        ca, ct_vec = self.ca(ct_vec.transpose(2, 1))\n",
    "#         print(f\"After self attention: ca.size: {ca.size()}, ct_vec.size: {ct_vec.size()}\")\n",
    "        # ct_vec shape: [batch_size, D1*hid1, num_cycle]\n",
    "        ct_vec = ct_vec.transpose(2, 1)\n",
    "        \n",
    "        \n",
    "        # input for 1d cnn: (batch_size, D1*hid1, num_cycle)\n",
    "        for i in range(1, self.pool2+self.npool2+1):\n",
    "            ct_vec = globals()[f'self.conv_block{i}'](ct_vec)\n",
    "        # ct_vec from final convolutional layer: (batch_size, self.in2*self.Lout)\n",
    "        ct_vec = ct_vec.view(ct_vec.size(0), -1)\n",
    "        # final_out: (batch_size, 1)\n",
    "        final_out = self.fc(ct_vec)\n",
    "        \n",
    "        return final_out.squeeze(), ta.squeeze(), ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d5ab99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for saving and loading of training history\n",
    "def save_data(D3_array, filename):\n",
    "    with open(filename,\"wb\") as dat_:\n",
    "        pickle.dump(D3_array,dat_)\n",
    "        \n",
    "def load_data(filename):\n",
    "    with open(filename,\"rb\") as ld:\n",
    "        x_temp = pickle.load(ld)\n",
    "    return x_temp\n",
    "\n",
    "def history_state_dict_add(num_cycles, modelname, n_ep, patience, rnn1, hid1, nh2, fil2, pool2, npool2, fsize2, psize2, lr):\n",
    "    history_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, f'{rnn1}_1D CNN', 'train_history'], \n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_nh_{nh2}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}.pkl')\n",
    "    state_dict_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, f'{rnn1}_1D CNN', 'model'], \n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_nh_{nh2}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}_state_dict.pth')\n",
    "    ta_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, f'{rnn1}_1D CNN', 'ta'],\n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_nh_{nh2}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}_ta.wb')\n",
    "    ca_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, f'{rnn1}_1D CNN', 'ca'],\n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_nh_{nh2}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}_ta.wb')\n",
    "    return history_add, state_dict_add, ta_add, ca_add\n",
    "\n",
    "def ta_per_batches_add(num_cycles, modelname, n_ep, patience, rnn1, hid1, nh2, fil2, pool2, npool2, fsize2, psize2, lr):\n",
    "    batch1_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, f'{rnn1}_1D CNN', 'ta_batch1'], \n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}.pkl')\n",
    "    batch2_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, f'{rnn1}_1D CNN', 'ta_batch2'], \n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}.pkl')\n",
    "    batch3_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, f'{rnn1}_1D CNN', 'ta_batch3'],\n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}.pkl')\n",
    "    fig_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, 'figures', 'ta'],\n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}.png')\n",
    "    return batch1_add, batch2_add, batch3_add, fig_add\n",
    "\n",
    "def ca_per_batches_add(num_cycles, modelname, n_ep, patience, rnn1, hid1, nh2, fil2, pool2, npool2, fsize2, psize2, lr):\n",
    "    batch1_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, f'{rnn1}_1D CNN', 'ca_batch1', f'{nh2} heads'], \n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}_n_head_{nh2}.pkl')\n",
    "    batch2_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, f'{rnn1}_1D CNN', 'ca_batch2', f'{nh2} heads'], \n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}_n_head_{nh2}.pkl')\n",
    "    batch3_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, f'{rnn1}_1D CNN', 'ca_batch3', f'{nh2} heads'],\n",
    "                                             filename = f'{rnn1}_hidden_{hid1}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}_n_head_{nh2}.pkl')\n",
    "\n",
    "    fig_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, 'figures', 'ca', f'{nh2} heads'],\n",
    "                                                 filename = f'{rnn1}_hidden_{hid1}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}_n_head_{nh2}_1th head.png')\n",
    "    fig_add = [fig_add]\n",
    "    if nh2>1:\n",
    "        for nh in range(1, nh2):\n",
    "            globals()[f'fig_add_{nh}'] = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname, 'figures', 'ca', f'{nh2} heads'],\n",
    "                                                     filename = f'{rnn1}_hidden_{hid1}_n_fil_{fil2}_pool_{pool2}_npool_{npool2}_fsize_{fsize2}_psize_{psize2}_lr_1_{int(1/lr)}_n_head_{nh2}_{nh+1}th head.png')\n",
    "            fig_add.append(globals()[f'fig_add_{nh}'])\n",
    "    return batch1_add, batch2_add, batch3_add, fig_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85992099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f963112d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN_TA_CA_1DCNN\n",
      "epoch = 10000, patience = 1000\n",
      "Test error under 100 in n_ep = 10000, n_head = 1: 210/324\n",
      "Test set error under 100 in all cases: 210/324\n"
     ]
    }
   ],
   "source": [
    "num_vars = 5\n",
    "modelname = 'RNN_TA_CA_1DCNN'\n",
    "rnns = ['LSTM', 'GRU', 'RNN']\n",
    "hids = [3, 5, 7]\n",
    "num_time = max_time\n",
    "num_cycles = 100\n",
    "# ep_pats = [[1000, 10], [2000, 20], [3000, 500]]\n",
    "ep_pats = [[10000, 1000]]\n",
    "lrs = [1e-4, 1e-3, 1e-2]\n",
    "num_heads = [1]\n",
    "\n",
    "num_fils = [3, 5, 7]\n",
    "pools = [1, 2]\n",
    "npools = [1, 2]\n",
    "fsize2 = 3\n",
    "psize2 = 2\n",
    "mids = [8, 4, 1]\n",
    "batch_size = 4\n",
    "\n",
    "case_num = 0\n",
    "case_test_under_100 = 0\n",
    "\n",
    "b1dl = globals()[f'b1_dataloader_{num_cycles}']\n",
    "b2dl = globals()[f'b2_dataloader_{num_cycles}']\n",
    "b3dl = globals()[f'b3_dataloader_{num_cycles}']\n",
    "dataloaders = [b1dl, b2dl, b3dl]\n",
    "batches = ['batch1', 'batch2', 'batch3']\n",
    "\n",
    "print(modelname)\n",
    "for [n_ep, patience] in ep_pats:\n",
    "    case_ep = 0\n",
    "    print(f\"epoch = {n_ep}, patience = {patience}\")\n",
    "    for num_head in num_heads:\n",
    "        case_ep_head = 0\n",
    "        case_test_under_100_n_ep_num_head = 0\n",
    "        for rnn1, hid1 in itertools.product(rnns, hids):\n",
    "            bi1 = False\n",
    "            if rnn1.startswith('Bi'):\n",
    "                bi1 = True\n",
    "            for pool2, npool2 in itertools.product(pools, npools):\n",
    "                for fil2, lr in itertools.product(num_fils, lrs):\n",
    "                    case_num += 1\n",
    "                    case_ep_head += 1;\n",
    "                    \n",
    "                    history_add, state_dict_add, ta_add, ca_add = history_state_dict_add(num_cycles, modelname, n_ep, patience,\n",
    "                                                                                rnn1, hid1, num_head, fil2, pool2, npool2, fsize2, psize2, lr)\n",
    "\n",
    "                    ta_batch1_add, ta_batch2_add, ta_batch3_add, ta_fig_add = ta_per_batches_add(num_cycles, modelname, n_ep, patience,\n",
    "                                                                    rnn1, hid1, num_head, fil2, pool2, npool2, fsize2, psize2, lr)\n",
    "\n",
    "                    ca_batch1_add, ca_batch2_add, ca_batch3_add, ca_fig_add = ca_per_batches_add(num_cycles, modelname, n_ep, patience,\n",
    "                                                                    rnn1, hid1, num_head, fil2, pool2, npool2, fsize2, psize2, lr)\n",
    "                    \n",
    "                    all_hid1 = hid1 * num_head\n",
    "                    \n",
    "                    model = globals()[modelname](num_vars, num_time, num_cycles, rnn1, bi1, all_hid1, num_head, fil2, pool2, npool2, fsize2, psize2, mids).to(device);\n",
    "                    _ = model.load_state_dict(load_data(state_dict_add));\n",
    "\n",
    "                    history = load_data(history_add);\n",
    "                    rmse = [round(history.iloc[-1, 0],2), round(history.iloc[-1, 1], 2), round(history.iloc[-1, 2], 2)];\n",
    "                    \n",
    "                    if round(history.iloc[-1, 2], 2)<100:\n",
    "#                         print(f'Epoch: {n_ep}, Pateince: {patience}, RNN: {rnn1}, Hidden: {hid1}, Pool: {pool2}, Nonpool: {npool2}, Num_head : {num_head}, Filter: {fil2}, lr: {lr}, RMSE: {rmse}')\n",
    "                        case_test_under_100 += 1\n",
    "                        case_test_under_100_n_ep_num_head += 1\n",
    "                        # Temporal code for 1000 epochs\n",
    "                        with torch.no_grad():\n",
    "                            i = 0\n",
    "                            fig, axs = plt.subplots(1, 3, figsize = (30, 8));\n",
    "                            for nh in range(num_head):\n",
    "                                globals()[f'fig{nh+2}'], globals()[f'ax{nh+2}'] = plt.subplots(1, 3, figsize = (30, 8));\n",
    "                            rows = 1\n",
    "                            cols = 3\n",
    "                            # ta: (batch_size, num_cycle, timestep)\n",
    "                            b1_ta = torch.empty(batch_size, num_cycles, num_time).to(device)\n",
    "                            b2_ta = torch.empty(batch_size, num_cycles, num_time).to(device)\n",
    "                            b3_ta = torch.empty(batch_size, num_cycles, num_time).to(device)\n",
    "                            tas = [b1_ta, b2_ta, b3_ta]\n",
    "\n",
    "                            # ca shape: [batch_size, num_heads, num_cycle, num_cycle]\n",
    "                            b1_ca = torch.empty(batch_size, num_head, num_cycles, num_cycles).to(device)\n",
    "                            b2_ca = torch.empty(batch_size, num_head, num_cycles, num_cycles).to(device)\n",
    "                            b3_ca = torch.empty(batch_size, num_head, num_cycles, num_cycles).to(device)\n",
    "                            cas = [b1_ca, b2_ca, b3_ca]\n",
    "\n",
    "                            for dataloader in dataloaders:\n",
    "                                for n, (inputs, targets) in enumerate(dataloader):\n",
    "                                    inputs = inputs.float().to(device)\n",
    "                                    targets = targets.float().to(device)\n",
    "                                    # ta: (batch_size, num_cycle, timestep)\n",
    "                                    # ca: (batch_size, num_heads, num_cycle, num_cycle)\n",
    "                                    yhat, ta, ca = model(inputs)\n",
    "\n",
    "                                    if len(ta.size()) !=3:\n",
    "                                        ta = torch.unsqueeze(ta, 0)\n",
    "                                    tas[i] = torch.cat((tas[i], ta), dim = 0)\n",
    "\n",
    "                                    if len(ca.size()) !=4:\n",
    "                                        ca = torch.unsqueeze(ca, 0)\n",
    "                                    cas[i] = torch.cat((cas[i], ca), dim = 0)\n",
    "\n",
    "                                tas[i] = torch.mean(tas[i][batch_size:], dim = 0).squeeze()\n",
    "                                tas[i] = tas[i].detach().cpu().numpy();\n",
    "\n",
    "                                cas[i] = torch.mean(cas[i][batch_size:], dim = 0).squeeze()\n",
    "                                cas[i] = cas[i].detach().cpu().numpy();\n",
    "#                                 print(cas[i].type())\n",
    "\n",
    "                                globals()[f'im_{i}'] = axs[i].imshow(tas[i], vmin = 0, vmax = 0.06, cmap = cm.gray);\n",
    "                                _ = axs[i].set_xlabel('Timestep', fontsize = 20);\n",
    "                                for t in axs[i].get_xticklabels():\n",
    "                                    t.set_fontsize(20);\n",
    "                                _ = axs[i].set_ylabel('Cycle', fontsize = 20);\n",
    "                                for t in axs[i].get_yticklabels():\n",
    "                                    t.set_fontsize(20);\n",
    "\n",
    "                                _ = axs[i].set_title(f'Batch {i+1} temporal attention score', fontsize = 25);\n",
    "\n",
    "                                for nh in range(num_head):\n",
    "#                                     print(cas[i][nh,:,:])\n",
    "                                    globals()[f'im{nh+2}_{i}'] = globals()[f'ax{nh+2}'][i].imshow(cas[i][nh,:,:], vmin = 0, vmax = 0.06, cmap = cm.gray);\n",
    "#                                     if num_head>1:\n",
    "#                                         globals()[f'im{nh+2}_{i}'] = globals()[f'ax{nh+2}'][i].imshow(cas[i][nh,:,:], vmin = 0, vmax = 0.06, cmap = cm.gray);\n",
    "#                                     else:\n",
    "#                                         globals()[f'im{nh+2}_{i}'] = globals()[f'ax{nh+2}'][i].imshow(cas[i], vmin = 0, vmax = 0.06, cmap = cm.gray);\n",
    "                                    _ = globals()[f'ax{nh+2}'][i].set_xlabel('Cycle', fontsize = 20);\n",
    "                                    for t in globals()[f'ax{nh+2}'][i].get_xticklabels():\n",
    "                                        t.set_fontsize(20);\n",
    "                                    _ = globals()[f'ax{nh+2}'][i].set_ylabel('Cycle', fontsize = 20);\n",
    "                                    for t in globals()[f'ax{nh+2}'][i].get_yticklabels():\n",
    "                                        t.set_fontsize(20);\n",
    "\n",
    "                                    _ = globals()[f'ax{nh+2}'][i].set_title(f'Batch {i+1} cyclic attention score', fontsize = 25);\n",
    "\n",
    "                                i+=1\n",
    "\n",
    "                            fig.subplots_adjust(right = 0.85);\n",
    "                            cbar_ax = fig.add_axes([0.88, 0.15, 0.02, 0.7]);\n",
    "                            for nh in range(num_head):\n",
    "                                globals()[f'fig{nh+2}'].subplots_adjust(right = 0.85);\n",
    "                                globals()[f'cbar_ax{nh+2}'] = globals()[f'fig{nh+2}'].add_axes([0.88, 0.15, 0.02, 0.7]);\n",
    "                            i -= 1\n",
    "                        #     globals()[f'divider_{i}'] = make_axes_locatable(axs[i]);\n",
    "                        #     globals()[f'cax_{i}'] = globals()[f'divider_{i}'].append_axes(\"right\", size=\"10%\", pad=0.05);\n",
    "\n",
    "\n",
    "                        #     globals()[f'cbar_{i}'] = plt.colorbar(globals()[f'im_{i}'], cax = globals()[f'cax_{i}']);\n",
    "                            globals()[f'cbar_{i}'] = plt.colorbar(globals()[f'im_{i}'], cax = cbar_ax);\n",
    "                            for nh in range(num_head):\n",
    "                                globals()[f'cbar{nh+2}_{i}'] = plt.colorbar(globals()[f'im{nh+2}_{i}'], cax = globals()[f'cbar_ax{nh+2}']);\n",
    "                                for t in globals()[f'cbar{nh+2}_{i}'].ax.get_yticklabels():\n",
    "                                    t.set_fontsize(20);\n",
    "\n",
    "                            for t in globals()[f'cbar_{i}'].ax.get_yticklabels():\n",
    "                                t.set_fontsize(20);\n",
    "\n",
    "\n",
    "                        #     plt.colorbar();\n",
    "\n",
    "                            _ = fig.suptitle(f'Epoch: {n_ep}, Patience: {patience}, RNN: {rnn1}, Hidden: {hid1}, Pool: {pool2}, Npool: {npool2}, Filter: {fil2}, lr: {lr}, RMSE:{rmse}, n_head = {num_head}', fontsize = 25);\n",
    "#                             fig.show();\n",
    "                            plt.close(fig);\n",
    "                            fig.savefig(ta_fig_add);\n",
    "\n",
    "                            for nh in range(num_head):\n",
    "                                _ = globals()[f'fig{nh+2}'].suptitle(f'Epoch: {n_ep}, Patience: {patience}, RNN: {rnn1}, Hidden: {hid1}, Pool: {pool2}, Npool: {npool2}, Filter: {fil2}, lr: {lr}, RMSE:{rmse}, n_head = {num_head}, {nh+1}th head', fontsize = 25);\n",
    "#                                 globals()[f'fig{nh+2}'].show();\n",
    "                                plt.close(globals()[f'fig{nh+2}']);\n",
    "                                globals()[f'fig{nh+2}'].savefig(ca_fig_add[nh]);\n",
    "\n",
    "#                             print(f\"{batches[i]} done\")\n",
    "#             print(f\"{n_ep},{num_head} done\")\n",
    "        print(f\"Test error under 100 in n_ep = {n_ep}, n_head = {num_head}: {case_test_under_100_n_ep_num_head}/{case_ep_head}\")  \n",
    "print(f\"Test set error under 100 in all cases: {case_test_under_100}/{case_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1e9782",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_add, state_dict_add, ta_add, ca_add = history_state_dict_add(num_cycles, modelname, n_ep, patience,\n",
    "                                                    rnn1, hid1, num_head, fil2, pool2, npool2, fsize2, psize2, lr)\n",
    "\n",
    "ta_batch1_add, ta_batch2_add, ta_batch3_add, ta_fig_add = ta_per_batches_add(num_cycles, modelname, n_ep, patience,\n",
    "                                        rnn1, hid1, num_head, fil2, pool2, npool2, fsize2, psize2, lr)\n",
    "\n",
    "ca_batch1_add, ca_batch2_add, ca_batch3_add, ca_fig_add = ca_per_batches_add(num_cycles, modelname, n_ep, patience,\n",
    "                                        rnn1, hid1, num_head, fil2, pool2, npool2, fsize2, psize2, lr)\n",
    "\n",
    "all_hid1 = hid1 * num_head\n",
    "\n",
    "model = globals()[modelname](num_vars, num_time, num_cycles, rnn1, bi1, all_hid1, num_head, fil2, pool2, npool2, fsize2, psize2, mids).to(device);\n",
    "_ = model.load_state_dict(load_data(state_dict_add));\n",
    "\n",
    "history = load_data(history_add);\n",
    "rmse = [round(history.iloc[-1, 0],2), round(history.iloc[-1, 1], 2), round(history.iloc[-1, 2], 2)];\n",
    "case_ep += 1;\n",
    "if round(history.iloc[-1, 2], 2)<100:\n",
    "#                     print(f'{n_ep}, {patience}, {rnn1}, {hid1}, {pool2}, {npool2}, {fil2}, {lr}, {rmse}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        fig, axs = plt.subplots(1, 3, figsize = (30, 8));\n",
    "        for nh in range(num_head):\n",
    "            globals()[f'fig{nh+2}'], globals()[f'ax{nh+2}'] = plt.subplots(1, 3, figsize = (30, 8));\n",
    "        rows = 1\n",
    "        cols = 3\n",
    "        # ta: (batch_size, num_cycle, timestep)\n",
    "        b1_ta = torch.empty(batch_size, num_cycles, num_time).to(device)\n",
    "        b2_ta = torch.empty(batch_size, num_cycles, num_time).to(device)\n",
    "        b3_ta = torch.empty(batch_size, num_cycles, num_time).to(device)\n",
    "        tas = [b1_ta, b2_ta, b3_ta]\n",
    "\n",
    "        # ca shape: [batch_size, num_heads, num_cycle, num_cycle]\n",
    "        b1_ca = torch.empty(batch_size, num_head, num_cycles, num_cycles).to(device)\n",
    "        b2_ca = torch.empty(batch_size, num_head, num_cycles, num_cycles).to(device)\n",
    "        b3_ca = torch.empty(batch_size, num_head, num_cycles, num_cycles).to(device)\n",
    "        cas = [b1_ca, b2_ca, b3_ca]\n",
    "\n",
    "        for dataloader in dataloaders:\n",
    "            for n, (inputs, targets) in enumerate(dataloader):\n",
    "                inputs = inputs.float().to(device)\n",
    "                targets = targets.float().to(device)\n",
    "                # ta: (batch_size, num_cycle, timestep)\n",
    "                # ca: (batch_size, num_heads, num_cycle, num_cycle)\n",
    "                yhat, ta, ca = model(inputs)\n",
    "\n",
    "                if len(ta.size()) !=3:\n",
    "                    ta = torch.unsqueeze(ta, 0)\n",
    "                tas[i] = torch.cat((tas[i], ta), dim = 0)\n",
    "\n",
    "                if len(ca.size()) !=4:\n",
    "                    ca = torch.unsqueeze(ca, 0)\n",
    "                cas[i] = torch.cat((cas[i], ca), dim = 0)\n",
    "\n",
    "            tas[i] = torch.mean(tas[i][batch_size:], dim = 0).squeeze();\n",
    "            tas[i] = tas[i].detach().cpu().numpy();\n",
    "\n",
    "            cas[i] = torch.mean(cas[i][batch_size:], dim = 0).squeeze();\n",
    "            cas[i] = cas[i].detach().cpu().numpy();\n",
    "        #                                 print(cas[i].type())\n",
    "\n",
    "            globals()[f'im_{i}'] = axs[i].imshow(tas[i], vmin = 0, vmax = 0.06, cmap = cm.gray);\n",
    "            _ = axs[i].set_xlabel('Timestep', fontsize = 20);\n",
    "            for t in axs[i].get_xticklabels():\n",
    "                t.set_fontsize(20);\n",
    "            _ = axs[i].set_ylabel('Cycle', fontsize = 20);\n",
    "            for t in axs[i].get_yticklabels():\n",
    "                t.set_fontsize(20);\n",
    "\n",
    "            _ = axs[i].set_title(f'Batch {i+1} temporal attention score', fontsize = 25);\n",
    "\n",
    "            for nh in range(num_head):\n",
    "        #                                     print(cas[i][nh,:,:])\n",
    "                globals()[f'im{nh+2}_{i}'] = globals()[f'ax{nh+2}'][i].imshow(cas[i][nh,:,:], vmin = 0, vmax = 0.06, cmap = cm.gray);\n",
    "                _ = globals()[f'ax{nh+2}'][i].set_xlabel('Cycle', fontsize = 20);\n",
    "                for t in globals()[f'ax{nh+2}'][i].get_xticklabels():\n",
    "                    t.set_fontsize(20);\n",
    "                _ = globals()[f'ax{nh+2}'][i].set_ylabel('Cycle', fontsize = 20);\n",
    "                for t in globals()[f'ax{nh+2}'][i].get_yticklabels():\n",
    "                    t.set_fontsize(20);\n",
    "\n",
    "                _ = globals()[f'ax{nh+2}'][i].set_title(f'Batch {i+1} cyclic attention score', fontsize = 25);\n",
    "\n",
    "            i+=1\n",
    "\n",
    "        fig.subplots_adjust(right = 0.85);\n",
    "        cbar_ax = fig.add_axes([0.88, 0.15, 0.02, 0.7]);\n",
    "        for nh in range(num_head):\n",
    "            globals()[f'fig{nh+2}'].subplots_adjust(right = 0.85);\n",
    "            globals()[f'cbar_ax{nh+2}'] = globals()[f'fig{nh+2}'].add_axes([0.88, 0.15, 0.02, 0.7]);\n",
    "        i -= 1\n",
    "        #     globals()[f'divider_{i}'] = make_axes_locatable(axs[i]);\n",
    "        #     globals()[f'cax_{i}'] = globals()[f'divider_{i}'].append_axes(\"right\", size=\"10%\", pad=0.05);\n",
    "\n",
    "\n",
    "        #     globals()[f'cbar_{i}'] = plt.colorbar(globals()[f'im_{i}'], cax = globals()[f'cax_{i}']);\n",
    "        globals()[f'cbar_{i}'] = plt.colorbar(globals()[f'im_{i}'], cax = cbar_ax);\n",
    "        for nh in range(num_head):\n",
    "            globals()[f'cbar{nh+2}_{i}'] = plt.colorbar(globals()[f'im{nh+2}_{i}'], cax = globals()[f'cbar_ax{nh+2}']);\n",
    "            for t in globals()[f'cbar{nh+2}_{i}'].ax.get_yticklabels():\n",
    "                t.set_fontsize(20);\n",
    "\n",
    "        for t in globals()[f'cbar_{i}'].ax.get_yticklabels():\n",
    "            t.set_fontsize(20);\n",
    "\n",
    "        _ = fig.suptitle(f'Epoch: {n_ep}, Patience: {patience}, RNN: {rnn1}, Hidden: {hid1}, Pool: {pool2}, Npool: {npool2}, Filter: {fil2}, lr: {lr}, RMSE:{rmse}, n_head = {num_head}', fontsize = 25);\n",
    "        plt.close(fig)\n",
    "        fig.savefig(ta_fig_add);\n",
    "\n",
    "        for nh in range(num_head):\n",
    "            _ = globals()[f'fig{nh+2}'].suptitle(f'Epoch: {n_ep}, Patience: {patience}, RNN: {rnn1}, Hidden: {hid1}, Pool: {pool2}, Npool: {npool2}, Filter: {fil2}, lr: {lr}, RMSE:{rmse}, n_head = {num_head}, {nh+1}th head', fontsize = 25);\n",
    "            plt.close(globals()[f'fig{nh+2}'])\n",
    "            globals()[f'fig{nh+2}'].savefig(ca_fig_add[nh]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06f529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_vars = 5\n",
    "\n",
    "modelname = 'RNN_TA_CA_1DCNN'\n",
    "rnns = ['LSTM', 'GRU', 'RNN']\n",
    "hids = [3, 5, 7]\n",
    "num_time = max_time\n",
    "num_cycles = 100\n",
    "ep_pats = [[1000, 10], [2000, 20], [3000, 500]]\n",
    "lrs = [1e-4, 1e-3, 1e-2]\n",
    "\n",
    "num_fils = [3, 5, 7]\n",
    "pools = [1, 2]\n",
    "npools = [1, 2]\n",
    "fsize2 = 3\n",
    "psize2 = 2\n",
    "mids = [8, 4, 1]\n",
    "\n",
    "for [n_ep, patience] in ep_pats:\n",
    "    for rnn1 in rnns:\n",
    "        bi1 = False\n",
    "        if rnn1.startswith('Bi'):\n",
    "            bi1 = True\n",
    "        for num_head in num_heads:\n",
    "            df = pd.DataFrame(columns = ['train rmse', 'val rmse', 'test rmse'])\n",
    "            for hid1 in hids:\n",
    "                for pool2, npool2 in itertools.product(pools, npools):\n",
    "                    print(f'Pooling: {pool2} layers, Nonpooling: {npool2} layers')\n",
    "                    for fil2, lr in itertools.product(num_fils, lrs):\n",
    "\n",
    "                        history_add, _, _, _ = history_state_dict_add(num_cycles, modelname, n_ep, patience,\n",
    "                                                                rnn1, hid1, num_head, fil2, pool2, npool2, fsize2, psize2, lr)\n",
    "                        history = load_data(history_add)\n",
    "    #                     print(history.iloc[-1, :])\n",
    "\n",
    "                        df.loc[f'{num_head}_{rnn1}_{hid1}_{fil2}_{pool2}_{npool2}_{fsize2}_{lr}'] = [history.iloc[-1, 0], history.iloc[-1, 1], history.iloc[-1, 2]]\n",
    "\n",
    "            df\n",
    "            df_add = find_directory(back_num = 0, foldername = [f'{num_cycles} cycles', f'Depth Test_col_{n_ep}_{patience}', modelname], \n",
    "                            filename = f'{num_head} heads_{modelname}_{rnn1}.csv')\n",
    "            df.to_csv(df_add)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
